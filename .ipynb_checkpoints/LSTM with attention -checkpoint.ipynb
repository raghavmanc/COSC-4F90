{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec03b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Libraries\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import models\n",
    "from numpy import array_equal\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e799bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Checking for tensorflow-GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae94f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unizipped Attack File\n",
      "Unizipped Validation File\n",
      "Unizipped Training File\n",
      "Normal Data     --->  train_normal_data\n",
      "Attack Data     --->  train_attack_data\n",
      "Validation Data --->  train_validation_data\n"
     ]
    }
   ],
   "source": [
    "#Unzipping Attack and Validation folders\n",
    "\n",
    "# Unzip Attack Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Attack_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Attack File')\n",
    "    \n",
    "# Unzip Validation Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Validation_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Validation File')\n",
    "    \n",
    "# Unzip Training Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Training_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Training File')\n",
    "\n",
    "# list holding normal training data\n",
    "train_normal_data = [] \n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path \n",
    "# list that holds attack vectors\n",
    "train_attack_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Attack_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_attack_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "for i in range(len(train_attack_data)):\n",
    "  for j in range(len(train_attack_data[i])):\n",
    "    train_attack_data[i][j] = int(train_attack_data[i][j])\n",
    "\n",
    "from pathlib import Path \n",
    "# list that holds validation vectors\n",
    "train_validation_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Validation_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_validation_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "# list that holds training vectors\n",
    "train_normal_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Training_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_normal_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "for i in range(len(train_normal_data)):\n",
    "  for j in range(len(train_normal_data[i])):\n",
    "    train_normal_data[i][j] = int(train_normal_data[i][j])\n",
    "print(\"Normal Data     --->  train_normal_data\")\n",
    "print(\"Attack Data     --->  train_attack_data\")\n",
    "print(\"Validation Data --->  train_validation_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3911a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating one hot vectors\n",
    "dict = {}\n",
    "# Feature size\n",
    "vocab_size = 341\n",
    "\n",
    "for x in range(vocab_size):\n",
    "    arr=[]\n",
    "    arr = [0 for i in range(vocab_size)] \n",
    "    arr[x] = 1\n",
    "    dict[x] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f926e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequence size\n",
    "n = 15\n",
    "#N-Gram\n",
    "m = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccf14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method receives a request array and returns one hot encoded version of that array\n",
    "def generate_one_hot(request):\n",
    "    temp = []\n",
    "    for sys_call in request:\n",
    "        temp.append(dict[int(sys_call)])\n",
    "    return temp\n",
    "\n",
    "# This method recevies a request array, start and  end of the request and populates x_train and y_train\n",
    "# with returned one hot version from the generate_trainSet method\n",
    "def split_request(source, target, request, start, end):\n",
    "    while(len(request)-start >= n+m):\n",
    "        source.append(generate_one_hot(request[start:end]))\n",
    "        start += m\n",
    "        end += m\n",
    "        target.append(generate_one_hot(request[start:end]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7a4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = [] \n",
    "\n",
    "# Populating x_train and y_train with sources and targets\n",
    "for i in range(len(train_normal_data)):\n",
    "    split_request(x_train, y_train, train_normal_data[i],0,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b212699c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29186"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4495cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[0:29000]\n",
    "y_train = y_train[0:29000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6aa4409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b8d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting x_train and y_train into np arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0578ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test data into training and testing sets\n",
    "x_train, x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.20,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5174fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23200, 15, 341)\n",
      "(5800, 15, 341)\n"
     ]
    }
   ],
   "source": [
    "#x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d31b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps_in = n\n",
    "n_features = vocab_size\n",
    "latentSpaceDimension = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58a8f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, verbose=0):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "    self.verbose= verbose\n",
    "\n",
    "  def call(self, query, values):\n",
    "    if self.verbose:\n",
    "      print('\\n******* Bahdanau Attention STARTS******')\n",
    "      print('query (decoder hidden state): (batch_size, hidden size) ', query.shape)\n",
    "      print('values (encoder all hidden state): (batch_size, max_len, hidden size) ', values.shape)\n",
    "\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    \n",
    "    if self.verbose:\n",
    "      print('query_with_time_axis:(batch_size, 1, hidden size) ', query_with_time_axis.shape)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "    if self.verbose:\n",
    "      print('score: (batch_size, max_length, 1) ',score.shape)\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    if self.verbose:\n",
    "      print('attention_weights: (batch_size, max_length, 1) ',attention_weights.shape)\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    if self.verbose:\n",
    "      print('context_vector before reduce_sum: (batch_size, max_length, hidden_size) ',context_vector.shape)\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    if self.verbose:\n",
    "      print('context_vector after reduce_sum: (batch_size, hidden_size) ',context_vector.shape)\n",
    "      print('\\n******* Bahdanau Attention ENDS******')\n",
    "    return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9b8ddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BahdanauAttention object at 0x00000228E6119490>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"bahdanau_attention_7\" (type BahdanauAttention).\n\nin user code:\n\n    File \"C:\\Users\\ragha\\AppData\\Local\\Temp\\ipykernel_17616\\3481614026.py\", line 32, in call  *\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n    TypeError: 'dict' object is not callable\n\n\nCall arguments received:\n  • query=tf.Tensor(shape=(None, 16), dtype=float64)\n  • values=tf.Tensor(shape=(None, 15, 16), dtype=float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_timesteps_in):\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# 3 pay attention\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# create the context vector by applying attention to \u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# decoder_outputs (last hidden state) + encoder_outputs (all hidden states)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(attention)\n\u001b[1;32m---> 61\u001b[0m     context_vector, attention_weights\u001b[38;5;241m=\u001b[39m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     63\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention context_vector: (batch size, units) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(context_vector\u001b[38;5;241m.\u001b[39mshape))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"bahdanau_attention_7\" (type BahdanauAttention).\n\nin user code:\n\n    File \"C:\\Users\\ragha\\AppData\\Local\\Temp\\ipykernel_17616\\3481614026.py\", line 32, in call  *\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n    TypeError: 'dict' object is not callable\n\n\nCall arguments received:\n  • query=tf.Tensor(shape=(None, 16), dtype=float64)\n  • values=tf.Tensor(shape=(None, 15, 16), dtype=float64)"
     ]
    }
   ],
   "source": [
    "verbose= 0 \n",
    "#See all debug messages\n",
    "\n",
    "batch_size=1\n",
    "if verbose:\n",
    "  print('***** Model Hyper Parameters *******')\n",
    "  print('latentSpaceDimension: ', latentSpaceDimension)\n",
    "  print('batch_size: ', batch_size)\n",
    "  print('sequence length: ', n_timesteps_in)\n",
    "  print('n_features: ', n_features)\n",
    "\n",
    "  print('\\n***** TENSOR DIMENSIONS *******')\n",
    "\n",
    "# The first part is encoder\n",
    "encoder_inputs = Input(shape=(n_timesteps_in, n_features), name='encoder_inputs')\n",
    "encoder_lstm = LSTM(latentSpaceDimension,return_sequences=True, return_state=True,  name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "if verbose:\n",
    "  print ('Encoder output shape: (batch size, sequence length, latentSpaceDimension) {}'.format(encoder_outputs.shape))\n",
    "  print ('Encoder Hidden state shape: (batch size, latentSpaceDimension) {}'.format(encoder_state_h.shape))\n",
    "  print ('Encoder Cell state shape: (batch size, latentSpaceDimension) {}'.format(encoder_state_c.shape))\n",
    "# initial context vector is the states of the encoder\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "if verbose:\n",
    "  print(encoder_states)\n",
    "# Set up the attention layer\n",
    "attention= BahdanauAttention(latentSpaceDimension, verbose=verbose)\n",
    "\n",
    "\n",
    "# Set up the decoder layers\n",
    "decoder_inputs = Input(shape=(1, (n_features+latentSpaceDimension)),name='decoder_inputs')\n",
    "decoder_lstm = LSTM(latentSpaceDimension,  return_state=True, name='decoder_lstm')\n",
    "decoder_dense = Dense(n_features, activation='softmax',  name='decoder_dense')\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "# 1 initial decoder's input data\n",
    "# Prepare initial decoder input data that just contains the start character \n",
    "# Note that we made it a constant one-hot-encoded in the model\n",
    "# that is, [1 0 0 0 0 0 0 0 0 0] is the first input for each loop\n",
    "# one-hot encoded zero(0) is the start symbol\n",
    "inputs = np.zeros((batch_size, 1, n_features))\n",
    "inputs[:, 0, 0] = 1 \n",
    "\n",
    "\n",
    "# 2 initial decoder's state\n",
    "# encoder's last hidden state + last cell state\n",
    "decoder_outputs = encoder_state_h\n",
    "states = encoder_states\n",
    "if verbose:\n",
    "  print('initial decoder inputs: ', inputs.shape)\n",
    "\n",
    "# decoder will only process one time step at a time.\n",
    "for _ in range(n_timesteps_in):\n",
    "\n",
    "    # 3 pay attention\n",
    "    # create the context vector by applying attention to \n",
    "    # decoder_outputs (last hidden state) + encoder_outputs (all hidden states)\n",
    "    print(attention)\n",
    "    context_vector, attention_weights=attention(decoder_outputs, encoder_outputs)\n",
    "    if verbose:\n",
    "      print(\"Attention context_vector: (batch size, units) {}\".format(context_vector.shape))\n",
    "      print(\"Attention weights : (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "      print('decoder_outputs: (batch_size,  latentSpaceDimension) ', decoder_outputs.shape )\n",
    "\n",
    "    context_vector = tf.expand_dims(context_vector, 1)\n",
    "    if verbose:\n",
    "      print('Reshaped context_vector: ', context_vector.shape )\n",
    "\n",
    "    # 4. concatenate the input + context vectore to find the next decoder's input\n",
    "    inputs = tf.concat([context_vector, inputs], axis=-1)\n",
    "    \n",
    "    if verbose:\n",
    "      print('After concat inputs: (batch_size, 1, n_features + hidden_size): ',inputs.shape )\n",
    "\n",
    "    # 5. passing the concatenated vector to the LSTM\n",
    "    # Run the decoder on one timestep with attended input and previous states\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(inputs,\n",
    "                                            initial_state=states)\n",
    "    #decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2]))\n",
    "  \n",
    "    outputs = decoder_dense(decoder_outputs)\n",
    "    # 6. Use the last hidden state for prediction the output\n",
    "    # save the current prediction\n",
    "    # we will concatenate all predictions later\n",
    "    outputs = tf.expand_dims(outputs, 1)\n",
    "    all_outputs.append(outputs)\n",
    "    # 7. Reinject the output (prediction) as inputs for the next loop iteration\n",
    "    # as well as update the states\n",
    "    inputs = outputs\n",
    "    states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# 8. After running Decoder for max time steps\n",
    "# we had created a predition list for the output sequence\n",
    "# convert the list to output array by Concatenating all predictions \n",
    "# such as [batch_size, timesteps, features]\n",
    "decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "# 9. Define and compile model \n",
    "model_encoder_decoder_Bahdanau_Attention = Model(encoder_inputs, \n",
    "                                                 decoder_outputs, name='model_encoder_decoder')\n",
    "model_encoder_decoder_Bahdanau_Attention.compile(optimizer='rmsprop', \n",
    "                                                 loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c7fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1806fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9dd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d46897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a6926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives one hot represetation and returns index where value = 1\n",
    "def one_hot_decode(arr):\n",
    "    for index,num in enumerate(arr):\n",
    "        if num == 1:\n",
    "            return index\n",
    "        \n",
    "# Receives an array to append to and a 3D-array that is one hot encoded      \n",
    "def decode(arr, three_d_array):\n",
    "    for seq in three_d_array:\n",
    "        temp = []\n",
    "        for one_hot in seq:\n",
    "            temp.append(one_hot_decode(one_hot))\n",
    "        arr.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodes prediction done by LSTM and stores it in arr.\n",
    "def prediction_decode(arr, prediction):\n",
    "    for seq in prediction:\n",
    "        predict_temp = []\n",
    "        for one_hot in seq:\n",
    "            predict_temp.append(argmax(one_hot))\n",
    "        arr.append(predict_temp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6da4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect match\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def calc_belu(target, prediction):\n",
    "    reference = []\n",
    "    candidate = []\n",
    "    reference.append(target)\n",
    "    candidate.extend(prediction)\n",
    "    return sentence_bleu(reference, candidate, weights=(0.2,0.3,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b349ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "decode(sources, x_test[0:20])\n",
    "\n",
    "targets = []\n",
    "decode(targets, y_test[0:20])\n",
    "\n",
    "predictions = []\n",
    "prediction_decode(predictions, model_encoder_decoder.predict( x_test[0:20], batch_size = batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f73e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size):\n",
    "    print(f\"Source:     {sources[i]}\")\n",
    "    print(f\"Target:     {targets[i]}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = []\n",
    "for i in range(batch_size):\n",
    "    temp_score.append(calc_belu(targets[i],predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(temp_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "\n",
    "for i in range(len(train_validation_data)-2300-1000):\n",
    "    print(f'{i+1}/{len(train_validation_data)-2300-1000}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    means.append(statistics.mean(calc_request_belu_score(train_validation_data[i],0,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_means = []\n",
    "\n",
    "for i in range((len(train_attack_data))):\n",
    "    print(f'{i+1}/{len(train_attack_data)}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    attack_means.append(statistics.mean(calc_request_belu_score(train_attack_data[i],0,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(means))\n",
    "print(statistics.mean(attack_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471d598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
