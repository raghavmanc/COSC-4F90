{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ec03b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Libraries\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import models\n",
    "from numpy import array_equal\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e799bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Checking for tensorflow-GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae94f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unizipped Attack File\n",
      "Unizipped Validation File\n",
      "Unizipped Training File\n",
      "Normal Data     --->  train_normal_data\n",
      "Attack Data     --->  train_attack_data\n",
      "Validation Data --->  train_validation_data\n"
     ]
    }
   ],
   "source": [
    "#Unzipping Attack and Validation folders\n",
    "\n",
    "# Unzip Attack Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Attack_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Attack File')\n",
    "    \n",
    "# Unzip Validation Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Validation_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Validation File')\n",
    "    \n",
    "# Unzip Training Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Training_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Training File')\n",
    "\n",
    "# list holding normal training data\n",
    "train_normal_data = [] \n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path \n",
    "# list that holds attack vectors\n",
    "train_attack_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Attack_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_attack_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "for i in range(len(train_attack_data)):\n",
    "  for j in range(len(train_attack_data[i])):\n",
    "    train_attack_data[i][j] = int(train_attack_data[i][j])\n",
    "\n",
    "from pathlib import Path \n",
    "# list that holds validation vectors\n",
    "train_validation_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Validation_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_validation_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "# list that holds training vectors\n",
    "train_normal_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Training_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_normal_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "for i in range(len(train_normal_data)):\n",
    "  for j in range(len(train_normal_data[i])):\n",
    "    train_normal_data[i][j] = int(train_normal_data[i][j])\n",
    "print(\"Normal Data     --->  train_normal_data\")\n",
    "print(\"Attack Data     --->  train_attack_data\")\n",
    "print(\"Validation Data --->  train_validation_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3911a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating one hot vectors\n",
    "dict = {}\n",
    "# Feature size\n",
    "vocab_size = 341\n",
    "\n",
    "for x in range(vocab_size):\n",
    "    arr=[]\n",
    "    arr = [0 for i in range(vocab_size)] \n",
    "    arr[x] = 1\n",
    "    dict[x] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f926e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequence size\n",
    "n = 15\n",
    "#N-Gram\n",
    "m = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccf14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method receives a request array and returns one hot encoded version of that array\n",
    "def generate_one_hot(request):\n",
    "    temp = []\n",
    "    for sys_call in request:\n",
    "        temp.append(dict[int(sys_call)])\n",
    "    return temp\n",
    "\n",
    "# This method recevies a request array, start and  end of the request and populates x_train and y_train\n",
    "# with returned one hot version from the generate_trainSet method\n",
    "def split_request(source, target, request, start, end):\n",
    "    while(len(request)-start >= n+m):\n",
    "        source.append(generate_one_hot(request[start:end]))\n",
    "        start += m\n",
    "        end += m\n",
    "        target.append(generate_one_hot(request[start:end]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7a4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = [] \n",
    "\n",
    "# Populating x_train and y_train with sources and targets\n",
    "for i in range(len(train_normal_data)):\n",
    "    split_request(x_train, y_train, train_normal_data[i],0,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4495cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[0:19000]\n",
    "y_train = y_train[0:19000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b8d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting x_train and y_train into np arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67791959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b0578ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test data into training and testing sets\n",
    "x_train, x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.10,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5174fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17100, 15, 341)\n",
      "(1900, 15, 341)\n"
     ]
    }
   ],
   "source": [
    "#x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d31b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps_in = n\n",
    "n_features = vocab_size\n",
    "latentSpaceDimension = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a77adb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hard_coded_decoder_input_model(batch_size):\n",
    "  # The first part is encoder\n",
    "  encoder_inputs = Input(shape=(n_timesteps_in, n_features), name='encoder_inputs')\n",
    "  encoder_lstm = LSTM(latentSpaceDimension, return_state=True,  name='encoder_lstm')\n",
    "  encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "  \n",
    "  # initial context vector is the states of the encoder\n",
    "  states = [state_h, state_c]\n",
    "  \n",
    "  # Set up the decoder layers\n",
    "  decoder_inputs = Input(shape=(1, n_features))\n",
    "  decoder_lstm = LSTM(latentSpaceDimension, return_sequences=True, \n",
    "                      return_state=True, name='decoder_lstm')\n",
    "  decoder_dense = Dense(n_features, activation='softmax',  name='decoder_dense')\n",
    "\n",
    "  all_outputs = []\n",
    "  # Prepare decoder input data that just contains the start character 0\n",
    "  # Note that we made it a constant one-hot-encoded in the model\n",
    "  # that is, [1 0 0 0 0 0 0 0 0 0] is the initial input for each loop\n",
    "  decoder_input_data = np.zeros((batch_size, 1, n_features))\n",
    "  decoder_input_data[:, 0, 0] = 1 #\n",
    "  \n",
    "  # that is, [1 0 0 0 0 0 0 0 0 0] is the initial input for each loop\n",
    "  inputs = decoder_input_data\n",
    "  # decoder will only process one timestep at a time.\n",
    "  for _ in range(n_timesteps_in):\n",
    "      # Run the decoder on one timestep\n",
    "      outputs, state_h, state_c = decoder_lstm(inputs,\n",
    "                                              initial_state=states)\n",
    "      outputs = decoder_dense(outputs)\n",
    "      # Store the current prediction (we will concatenate all predictions later)\n",
    "      all_outputs.append(outputs)\n",
    "      # Reinject the outputs as inputs for the next loop iteration\n",
    "      # as well as update the states\n",
    "      inputs = outputs\n",
    "      states = [state_h, state_c]\n",
    "\n",
    "  # Concatenate all predictions such as [batch_size, timesteps, features]\n",
    "  decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "  # Define and compile model \n",
    "  model = Model(encoder_inputs, decoder_outputs, name='model_encoder_decoder')\n",
    "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10fc7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "model_encoder_decoder=create_hard_coded_decoder_input_model(batch_size=batch_size)\n",
    "#model_encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f59d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57bdfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Function to Train & Test  given model (Early Stopping monitor 'val_loss')\n",
    "def train_test(model, X_train, y_train , X_test, y_test, epochs=500, batch_size=batch_size, patience=5,verbose=0):\n",
    "\t# patient early stopping\n",
    "\tes = EarlyStopping(monitor='val_accuracy', mode='max',  verbose=1, patience=patience)\n",
    "\t#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\t# train model\n",
    "\tprint('training for ',epochs,' epochs begins with EarlyStopping(monitor= val_accuracy, patience=',patience,')....')\n",
    "\thistory=model.fit(X_train, y_train, validation_split= 0.1, epochs=epochs,batch_size=batch_size, verbose=verbose, callbacks=[es])\n",
    "\tprint(epochs,' epoch training finished...')\n",
    "\n",
    "\t# report training\n",
    "\t# list all data in history\n",
    "\t#print(history.history.keys())\n",
    "\t# evaluate the model\n",
    "\t_, train_acc = model.evaluate(X_train, y_train, batch_size=batch_size, verbose=0)\n",
    "\t_, test_acc = model.evaluate(X_test, \ty_test, batch_size=batch_size, verbose=0)\n",
    "\tprint('\\nPREDICTION ACCURACY (%):')\n",
    "\tprint('Train: %.3f, Test: %.3f' % (train_acc*100, test_acc*100))\n",
    "\t# summarize history for accuracy\n",
    "\tplt.plot(history.history['accuracy'])\n",
    "\tplt.plot(history.history['val_accuracy'])\n",
    "\tplt.title(model.name+' accuracy')\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\tplt.show()\n",
    "\t# summarize history for loss\n",
    "\tplt.plot(history.history['loss'])\n",
    "\tplt.plot(history.history['val_loss'])\n",
    "\tplt.title(model.name+' loss')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\tplt.show()\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b36eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, verbose=0):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "    self.verbose= verbose\n",
    "\n",
    "  def call(self, query, values):\n",
    "    if self.verbose:\n",
    "      print('\\n******* Bahdanau Attention STARTS******')\n",
    "      print('query (decoder hidden state): (batch_size, hidden size) ', query.shape)\n",
    "      print('values (encoder all hidden state): (batch_size, max_len, hidden size) ', values.shape)\n",
    "\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    \n",
    "    if self.verbose:\n",
    "      print('query_with_time_axis:(batch_size, 1, hidden size) ', query_with_time_axis.shape)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "    if self.verbose:\n",
    "      print('score: (batch_size, max_length, 1) ',score.shape)\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    if self.verbose:\n",
    "      print('attention_weights: (batch_size, max_length, 1) ',attention_weights.shape)\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    if self.verbose:\n",
    "      print('context_vector before reduce_sum: (batch_size, max_length, hidden_size) ',context_vector.shape)\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    if self.verbose:\n",
    "      print('context_vector after reduce_sum: (batch_size, hidden_size) ',context_vector.shape)\n",
    "      print('\\n******* Bahdanau Attention ENDS******')\n",
    "    return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90392377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "171/171 [==============================] - 30s 33ms/step - loss: 2.5328 - accuracy: 0.3330\n",
      "Epoch 2/500\n",
      "171/171 [==============================] - 5s 32ms/step - loss: 1.9763 - accuracy: 0.4303\n",
      "Epoch 3/500\n",
      "171/171 [==============================] - 5s 32ms/step - loss: 1.8632 - accuracy: 0.4430\n",
      "Epoch 4/500\n",
      "171/171 [==============================] - 5s 32ms/step - loss: 1.8078 - accuracy: 0.4480\n",
      "Epoch 5/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 1.7690 - accuracy: 0.4534\n",
      "Epoch 6/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.7405 - accuracy: 0.4580\n",
      "Epoch 7/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 1.7189 - accuracy: 0.4603\n",
      "Epoch 8/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.6971 - accuracy: 0.4644\n",
      "Epoch 9/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 1.6765 - accuracy: 0.4678\n",
      "Epoch 10/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 1.6594 - accuracy: 0.4715\n",
      "Epoch 11/500\n",
      "171/171 [==============================] - 8s 45ms/step - loss: 1.6416 - accuracy: 0.4745\n",
      "Epoch 12/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 1.6241 - accuracy: 0.4786\n",
      "Epoch 13/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 1.6021 - accuracy: 0.4835\n",
      "Epoch 14/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 1.5815 - accuracy: 0.4894\n",
      "Epoch 15/500\n",
      "171/171 [==============================] - 8s 45ms/step - loss: 1.5555 - accuracy: 0.4977\n",
      "Epoch 16/500\n",
      "171/171 [==============================] - 40s 226ms/step - loss: 1.5334 - accuracy: 0.5048\n",
      "Epoch 17/500\n",
      "171/171 [==============================] - 40s 235ms/step - loss: 1.5137 - accuracy: 0.5105\n",
      "Epoch 18/500\n",
      "171/171 [==============================] - 35s 207ms/step - loss: 1.4990 - accuracy: 0.5156\n",
      "Epoch 19/500\n",
      "171/171 [==============================] - 25s 148ms/step - loss: 1.4813 - accuracy: 0.5189\n",
      "Epoch 20/500\n",
      "171/171 [==============================] - 31s 180ms/step - loss: 1.4673 - accuracy: 0.5241\n",
      "Epoch 21/500\n",
      "171/171 [==============================] - 32s 186ms/step - loss: 1.4518 - accuracy: 0.5279\n",
      "Epoch 22/500\n",
      "171/171 [==============================] - 27s 159ms/step - loss: 1.4410 - accuracy: 0.5317\n",
      "Epoch 23/500\n",
      "171/171 [==============================] - 26s 152ms/step - loss: 1.4264 - accuracy: 0.5354\n",
      "Epoch 24/500\n",
      "171/171 [==============================] - 21s 125ms/step - loss: 1.4130 - accuracy: 0.5391\n",
      "Epoch 25/500\n",
      "171/171 [==============================] - 24s 141ms/step - loss: 1.4013 - accuracy: 0.5431\n",
      "Epoch 26/500\n",
      "171/171 [==============================] - 24s 142ms/step - loss: 1.3898 - accuracy: 0.5468\n",
      "Epoch 27/500\n",
      "171/171 [==============================] - 24s 143ms/step - loss: 1.3798 - accuracy: 0.5468\n",
      "Epoch 28/500\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.3686 - accuracy: 0.5512\n",
      "Epoch 29/500\n",
      "171/171 [==============================] - 24s 124ms/step - loss: 1.3579 - accuracy: 0.5553\n",
      "Epoch 30/500\n",
      "171/171 [==============================] - 21s 124ms/step - loss: 1.3484 - accuracy: 0.5568\n",
      "Epoch 31/500\n",
      "171/171 [==============================] - 21s 124ms/step - loss: 1.3374 - accuracy: 0.5595\n",
      "Epoch 32/500\n",
      "171/171 [==============================] - 24s 143ms/step - loss: 1.3273 - accuracy: 0.5620\n",
      "Epoch 33/500\n",
      "171/171 [==============================] - 24s 142ms/step - loss: 1.3178 - accuracy: 0.5649\n",
      "Epoch 34/500\n",
      "171/171 [==============================] - 24s 141ms/step - loss: 1.3114 - accuracy: 0.5673\n",
      "Epoch 35/500\n",
      "171/171 [==============================] - 21s 125ms/step - loss: 1.2994 - accuracy: 0.5712\n",
      "Epoch 36/500\n",
      "171/171 [==============================] - 24s 141ms/step - loss: 1.2909 - accuracy: 0.5719\n",
      "Epoch 37/500\n",
      "171/171 [==============================] - 21s 124ms/step - loss: 1.2803 - accuracy: 0.5749\n",
      "Epoch 38/500\n",
      "171/171 [==============================] - 24s 142ms/step - loss: 1.2729 - accuracy: 0.5781\n",
      "Epoch 39/500\n",
      "171/171 [==============================] - 25s 145ms/step - loss: 1.2642 - accuracy: 0.5805\n",
      "Epoch 40/500\n",
      "171/171 [==============================] - 24s 142ms/step - loss: 1.2535 - accuracy: 0.5841\n",
      "Epoch 41/500\n",
      "171/171 [==============================] - 24s 141ms/step - loss: 1.2448 - accuracy: 0.5869\n",
      "Epoch 42/500\n",
      "171/171 [==============================] - 24s 141ms/step - loss: 1.2371 - accuracy: 0.5880\n",
      "Epoch 43/500\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.2285 - accuracy: 0.5914\n",
      "Epoch 44/500\n",
      "171/171 [==============================] - 24s 141ms/step - loss: 1.2199 - accuracy: 0.5944\n",
      "Epoch 45/500\n",
      "171/171 [==============================] - 25s 145ms/step - loss: 1.2123 - accuracy: 0.5967\n",
      "Epoch 46/500\n",
      "171/171 [==============================] - 25s 145ms/step - loss: 1.2070 - accuracy: 0.5978\n",
      "Epoch 47/500\n",
      "171/171 [==============================] - 29s 170ms/step - loss: 1.1970 - accuracy: 0.6005\n",
      "Epoch 48/500\n",
      "171/171 [==============================] - 35s 204ms/step - loss: 1.1905 - accuracy: 0.6027\n",
      "Epoch 49/500\n",
      "171/171 [==============================] - 32s 189ms/step - loss: 1.1843 - accuracy: 0.6047\n",
      "Epoch 50/500\n",
      "171/171 [==============================] - 28s 163ms/step - loss: 1.1765 - accuracy: 0.6063\n",
      "Epoch 51/500\n",
      "171/171 [==============================] - 30s 177ms/step - loss: 1.1689 - accuracy: 0.6088\n",
      "Epoch 52/500\n",
      "171/171 [==============================] - 24s 144ms/step - loss: 1.1620 - accuracy: 0.6108\n",
      "Epoch 53/500\n",
      "171/171 [==============================] - 22s 129ms/step - loss: 1.1567 - accuracy: 0.6125\n",
      "Epoch 54/500\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.1494 - accuracy: 0.6143\n",
      "Epoch 55/500\n",
      "171/171 [==============================] - 24s 142ms/step - loss: 1.1418 - accuracy: 0.6156\n",
      "Epoch 56/500\n",
      "171/171 [==============================] - 20s 117ms/step - loss: 1.1344 - accuracy: 0.6183\n",
      "Epoch 57/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 1.1293 - accuracy: 0.6197\n",
      "Epoch 58/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.1219 - accuracy: 0.6218\n",
      "Epoch 59/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.1146 - accuracy: 0.6243\n",
      "Epoch 60/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.1099 - accuracy: 0.6257\n",
      "Epoch 61/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.1035 - accuracy: 0.6266\n",
      "Epoch 62/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.0993 - accuracy: 0.6282\n",
      "Epoch 63/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0926 - accuracy: 0.6307\n",
      "Epoch 64/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0869 - accuracy: 0.6318\n",
      "Epoch 65/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0814 - accuracy: 0.6342\n",
      "Epoch 66/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0759 - accuracy: 0.6348\n",
      "Epoch 67/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.0710 - accuracy: 0.6371\n",
      "Epoch 68/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.0635 - accuracy: 0.6380\n",
      "Epoch 69/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.0576 - accuracy: 0.6400\n",
      "Epoch 70/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0530 - accuracy: 0.6414\n",
      "Epoch 71/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0496 - accuracy: 0.6427\n",
      "Epoch 72/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 1.0435 - accuracy: 0.6444\n",
      "Epoch 73/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 1.0388 - accuracy: 0.6450\n",
      "Epoch 74/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 1.0337 - accuracy: 0.6476\n",
      "Epoch 75/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.0263 - accuracy: 0.6500\n",
      "Epoch 76/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0215 - accuracy: 0.6516\n",
      "Epoch 77/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.0138 - accuracy: 0.6542\n",
      "Epoch 78/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 1.0101 - accuracy: 0.6545\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 6s 36ms/step - loss: 1.0034 - accuracy: 0.6574\n",
      "Epoch 80/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 1.0017 - accuracy: 0.6578\n",
      "Epoch 81/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9959 - accuracy: 0.6595\n",
      "Epoch 82/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9925 - accuracy: 0.6604\n",
      "Epoch 83/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9861 - accuracy: 0.6623\n",
      "Epoch 84/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.9815 - accuracy: 0.6636\n",
      "Epoch 85/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9759 - accuracy: 0.6648\n",
      "Epoch 86/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9737 - accuracy: 0.6666\n",
      "Epoch 87/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9654 - accuracy: 0.6688\n",
      "Epoch 88/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.9640 - accuracy: 0.6681\n",
      "Epoch 89/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9592 - accuracy: 0.6703\n",
      "Epoch 90/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.9543 - accuracy: 0.6720\n",
      "Epoch 91/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9498 - accuracy: 0.6735\n",
      "Epoch 92/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.9458 - accuracy: 0.6751\n",
      "Epoch 93/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9416 - accuracy: 0.6757\n",
      "Epoch 94/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9342 - accuracy: 0.6782\n",
      "Epoch 95/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9327 - accuracy: 0.6786\n",
      "Epoch 96/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.9271 - accuracy: 0.6803\n",
      "Epoch 97/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9262 - accuracy: 0.6802\n",
      "Epoch 98/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9196 - accuracy: 0.6822\n",
      "Epoch 99/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9170 - accuracy: 0.6843\n",
      "Epoch 100/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9100 - accuracy: 0.6865\n",
      "Epoch 101/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.9059 - accuracy: 0.6878\n",
      "Epoch 102/500\n",
      "171/171 [==============================] - 8s 45ms/step - loss: 0.9028 - accuracy: 0.6881\n",
      "Epoch 103/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.8988 - accuracy: 0.6902\n",
      "Epoch 104/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.8960 - accuracy: 0.6893\n",
      "Epoch 105/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8904 - accuracy: 0.6919\n",
      "Epoch 106/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.8876 - accuracy: 0.6935\n",
      "Epoch 107/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.8832 - accuracy: 0.6937\n",
      "Epoch 108/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.8796 - accuracy: 0.6960\n",
      "Epoch 109/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8750 - accuracy: 0.6968\n",
      "Epoch 110/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8718 - accuracy: 0.6971\n",
      "Epoch 111/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.8673 - accuracy: 0.6987\n",
      "Epoch 112/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.8652 - accuracy: 0.6994\n",
      "Epoch 113/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8613 - accuracy: 0.7016\n",
      "Epoch 114/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8575 - accuracy: 0.7031\n",
      "Epoch 115/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8530 - accuracy: 0.7032\n",
      "Epoch 116/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.8505 - accuracy: 0.7045\n",
      "Epoch 117/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8451 - accuracy: 0.7066\n",
      "Epoch 118/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8437 - accuracy: 0.7064\n",
      "Epoch 119/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8396 - accuracy: 0.7081\n",
      "Epoch 120/500\n",
      "171/171 [==============================] - 7s 43ms/step - loss: 0.8371 - accuracy: 0.7087\n",
      "Epoch 121/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8327 - accuracy: 0.7111\n",
      "Epoch 122/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.8308 - accuracy: 0.7112\n",
      "Epoch 123/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.8258 - accuracy: 0.7125\n",
      "Epoch 124/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.8218 - accuracy: 0.7144\n",
      "Epoch 125/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8180 - accuracy: 0.7151\n",
      "Epoch 126/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.8139 - accuracy: 0.7164\n",
      "Epoch 127/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.8134 - accuracy: 0.7158\n",
      "Epoch 128/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.8085 - accuracy: 0.7177\n",
      "Epoch 129/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.8039 - accuracy: 0.7197\n",
      "Epoch 130/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.8012 - accuracy: 0.7207\n",
      "Epoch 131/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.7990 - accuracy: 0.7215\n",
      "Epoch 132/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.7967 - accuracy: 0.7225\n",
      "Epoch 133/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.7923 - accuracy: 0.7244\n",
      "Epoch 134/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.7899 - accuracy: 0.7253\n",
      "Epoch 135/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.7866 - accuracy: 0.7252\n",
      "Epoch 136/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.7822 - accuracy: 0.7271\n",
      "Epoch 137/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.7810 - accuracy: 0.7277\n",
      "Epoch 138/500\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 0.7783 - accuracy: 0.7285\n",
      "Epoch 139/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.7749 - accuracy: 0.7288\n",
      "Epoch 140/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7717 - accuracy: 0.7302\n",
      "Epoch 141/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7697 - accuracy: 0.7319\n",
      "Epoch 142/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.7654 - accuracy: 0.7319\n",
      "Epoch 143/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7644 - accuracy: 0.7330\n",
      "Epoch 144/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7601 - accuracy: 0.7348\n",
      "Epoch 145/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7575 - accuracy: 0.7345\n",
      "Epoch 146/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7545 - accuracy: 0.7367\n",
      "Epoch 147/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7518 - accuracy: 0.7371\n",
      "Epoch 148/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.7475 - accuracy: 0.7388\n",
      "Epoch 149/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7458 - accuracy: 0.7390\n",
      "Epoch 150/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7439 - accuracy: 0.7400\n",
      "Epoch 151/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7400 - accuracy: 0.7415\n",
      "Epoch 152/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7408 - accuracy: 0.7409\n",
      "Epoch 153/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.7345 - accuracy: 0.7434\n",
      "Epoch 154/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7304 - accuracy: 0.7443\n",
      "Epoch 155/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.7297 - accuracy: 0.7450\n",
      "Epoch 156/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7259 - accuracy: 0.7467\n",
      "Epoch 157/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7243 - accuracy: 0.7468\n",
      "Epoch 158/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 7s 41ms/step - loss: 0.7198 - accuracy: 0.7490\n",
      "Epoch 159/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.7181 - accuracy: 0.7492\n",
      "Epoch 160/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.7163 - accuracy: 0.7492\n",
      "Epoch 161/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.7152 - accuracy: 0.7497\n",
      "Epoch 162/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.7138 - accuracy: 0.7513\n",
      "Epoch 163/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.7107 - accuracy: 0.7516\n",
      "Epoch 164/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.7061 - accuracy: 0.7537\n",
      "Epoch 165/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.7015 - accuracy: 0.7553\n",
      "Epoch 166/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.7003 - accuracy: 0.7551\n",
      "Epoch 167/500\n",
      "171/171 [==============================] - 8s 45ms/step - loss: 0.7014 - accuracy: 0.7550\n",
      "Epoch 168/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.6986 - accuracy: 0.7567\n",
      "Epoch 169/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6932 - accuracy: 0.7580\n",
      "Epoch 170/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6923 - accuracy: 0.7578\n",
      "Epoch 171/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6895 - accuracy: 0.7592\n",
      "Epoch 172/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6877 - accuracy: 0.7605\n",
      "Epoch 173/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6843 - accuracy: 0.7608\n",
      "Epoch 174/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6843 - accuracy: 0.7620\n",
      "Epoch 175/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.6864 - accuracy: 0.7613\n",
      "Epoch 176/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.6801 - accuracy: 0.7645\n",
      "Epoch 177/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.6774 - accuracy: 0.7638\n",
      "Epoch 178/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6737 - accuracy: 0.7648\n",
      "Epoch 179/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6722 - accuracy: 0.7662\n",
      "Epoch 180/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6699 - accuracy: 0.7674\n",
      "Epoch 181/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6682 - accuracy: 0.7681\n",
      "Epoch 182/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6667 - accuracy: 0.7672\n",
      "Epoch 183/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6624 - accuracy: 0.7693\n",
      "Epoch 184/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6605 - accuracy: 0.7706\n",
      "Epoch 185/500\n",
      "171/171 [==============================] - 8s 49ms/step - loss: 0.6614 - accuracy: 0.7697\n",
      "Epoch 186/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.6589 - accuracy: 0.7710\n",
      "Epoch 187/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.6562 - accuracy: 0.7712\n",
      "Epoch 188/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6551 - accuracy: 0.7718\n",
      "Epoch 189/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6528 - accuracy: 0.7724\n",
      "Epoch 190/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6497 - accuracy: 0.7743\n",
      "Epoch 191/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6482 - accuracy: 0.7743\n",
      "Epoch 192/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.6451 - accuracy: 0.7758\n",
      "Epoch 193/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.6456 - accuracy: 0.7749\n",
      "Epoch 194/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6423 - accuracy: 0.7765\n",
      "Epoch 195/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6400 - accuracy: 0.7767\n",
      "Epoch 196/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6406 - accuracy: 0.7772\n",
      "Epoch 197/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.6389 - accuracy: 0.7787\n",
      "Epoch 198/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.6375 - accuracy: 0.7786\n",
      "Epoch 199/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6330 - accuracy: 0.7803\n",
      "Epoch 200/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6313 - accuracy: 0.7805\n",
      "Epoch 201/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.6309 - accuracy: 0.7809\n",
      "Epoch 202/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.6304 - accuracy: 0.7808\n",
      "Epoch 203/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6280 - accuracy: 0.7819\n",
      "Epoch 204/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6268 - accuracy: 0.7824\n",
      "Epoch 205/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6196 - accuracy: 0.7844\n",
      "Epoch 206/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6220 - accuracy: 0.7837\n",
      "Epoch 207/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.6205 - accuracy: 0.7850\n",
      "Epoch 208/500\n",
      "171/171 [==============================] - 7s 43ms/step - loss: 0.6193 - accuracy: 0.7853\n",
      "Epoch 209/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.6157 - accuracy: 0.7869\n",
      "Epoch 210/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.6138 - accuracy: 0.7883\n",
      "Epoch 211/500\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.6105 - accuracy: 0.7883\n",
      "Epoch 212/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6113 - accuracy: 0.7884\n",
      "Epoch 213/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6075 - accuracy: 0.7902\n",
      "Epoch 214/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.6086 - accuracy: 0.7887\n",
      "Epoch 215/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6054 - accuracy: 0.7899\n",
      "Epoch 216/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6064 - accuracy: 0.7897\n",
      "Epoch 217/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6023 - accuracy: 0.7919\n",
      "Epoch 218/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5988 - accuracy: 0.7928\n",
      "Epoch 219/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.6006 - accuracy: 0.7921\n",
      "Epoch 220/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.6009 - accuracy: 0.7917\n",
      "Epoch 221/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5973 - accuracy: 0.7933\n",
      "Epoch 222/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5956 - accuracy: 0.7928\n",
      "Epoch 223/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5940 - accuracy: 0.7947\n",
      "Epoch 224/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5931 - accuracy: 0.7947\n",
      "Epoch 225/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5944 - accuracy: 0.7941\n",
      "Epoch 226/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5892 - accuracy: 0.7963\n",
      "Epoch 227/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5885 - accuracy: 0.7962\n",
      "Epoch 228/500\n",
      "171/171 [==============================] - 9s 53ms/step - loss: 0.5876 - accuracy: 0.7966\n",
      "Epoch 229/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5828 - accuracy: 0.7987\n",
      "Epoch 230/500\n",
      "171/171 [==============================] - 8s 46ms/step - loss: 0.5841 - accuracy: 0.7973\n",
      "Epoch 231/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5850 - accuracy: 0.7975\n",
      "Epoch 232/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5840 - accuracy: 0.7987\n",
      "Epoch 233/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5801 - accuracy: 0.7993\n",
      "Epoch 234/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5786 - accuracy: 0.7999\n",
      "Epoch 235/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5780 - accuracy: 0.8003\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5758 - accuracy: 0.8016\n",
      "Epoch 237/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5749 - accuracy: 0.8012\n",
      "Epoch 238/500\n",
      "171/171 [==============================] - 8s 46ms/step - loss: 0.5746 - accuracy: 0.8024\n",
      "Epoch 239/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5731 - accuracy: 0.8015\n",
      "Epoch 240/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5716 - accuracy: 0.8024\n",
      "Epoch 241/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5711 - accuracy: 0.8029\n",
      "Epoch 242/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.5702 - accuracy: 0.8033\n",
      "Epoch 243/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5661 - accuracy: 0.8041\n",
      "Epoch 244/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5636 - accuracy: 0.8057\n",
      "Epoch 245/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5638 - accuracy: 0.8057\n",
      "Epoch 246/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.5623 - accuracy: 0.8067\n",
      "Epoch 247/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5631 - accuracy: 0.8065\n",
      "Epoch 248/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5604 - accuracy: 0.8064\n",
      "Epoch 249/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5591 - accuracy: 0.8073\n",
      "Epoch 250/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5590 - accuracy: 0.8075\n",
      "Epoch 251/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5580 - accuracy: 0.8070\n",
      "Epoch 252/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5565 - accuracy: 0.8081\n",
      "Epoch 253/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5545 - accuracy: 0.8087\n",
      "Epoch 254/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5541 - accuracy: 0.8085\n",
      "Epoch 255/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5498 - accuracy: 0.8102\n",
      "Epoch 256/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5506 - accuracy: 0.8113\n",
      "Epoch 257/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5531 - accuracy: 0.8103\n",
      "Epoch 258/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5476 - accuracy: 0.8118\n",
      "Epoch 259/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5469 - accuracy: 0.8121\n",
      "Epoch 260/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5473 - accuracy: 0.8115\n",
      "Epoch 261/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5446 - accuracy: 0.8126\n",
      "Epoch 262/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.5448 - accuracy: 0.8125\n",
      "Epoch 263/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.5416 - accuracy: 0.8125\n",
      "Epoch 264/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5413 - accuracy: 0.8133\n",
      "Epoch 265/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5386 - accuracy: 0.8143\n",
      "Epoch 266/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5377 - accuracy: 0.8146\n",
      "Epoch 267/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5384 - accuracy: 0.8144\n",
      "Epoch 268/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.5358 - accuracy: 0.8158\n",
      "Epoch 269/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.5354 - accuracy: 0.8163\n",
      "Epoch 270/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.5325 - accuracy: 0.8174\n",
      "Epoch 271/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.5345 - accuracy: 0.8155\n",
      "Epoch 272/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5300 - accuracy: 0.8178\n",
      "Epoch 273/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5304 - accuracy: 0.8176\n",
      "Epoch 274/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5315 - accuracy: 0.8171\n",
      "Epoch 275/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.5318 - accuracy: 0.8175\n",
      "Epoch 276/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5289 - accuracy: 0.8178\n",
      "Epoch 277/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5249 - accuracy: 0.8195\n",
      "Epoch 278/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5268 - accuracy: 0.8190\n",
      "Epoch 279/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5269 - accuracy: 0.8188\n",
      "Epoch 280/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5234 - accuracy: 0.8209\n",
      "Epoch 281/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5222 - accuracy: 0.8208\n",
      "Epoch 282/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5208 - accuracy: 0.8215\n",
      "Epoch 283/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5224 - accuracy: 0.8207\n",
      "Epoch 284/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5175 - accuracy: 0.8223\n",
      "Epoch 285/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5204 - accuracy: 0.8210\n",
      "Epoch 286/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5194 - accuracy: 0.8214\n",
      "Epoch 287/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5198 - accuracy: 0.8219\n",
      "Epoch 288/500\n",
      "171/171 [==============================] - 7s 37ms/step - loss: 0.5204 - accuracy: 0.8211\n",
      "Epoch 289/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.5152 - accuracy: 0.8233\n",
      "Epoch 290/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5163 - accuracy: 0.8237\n",
      "Epoch 291/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5164 - accuracy: 0.8229\n",
      "Epoch 292/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5133 - accuracy: 0.8242\n",
      "Epoch 293/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.5140 - accuracy: 0.8243\n",
      "Epoch 294/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5107 - accuracy: 0.8252\n",
      "Epoch 295/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5109 - accuracy: 0.8250\n",
      "Epoch 296/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5085 - accuracy: 0.8257\n",
      "Epoch 297/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5107 - accuracy: 0.8253\n",
      "Epoch 298/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5077 - accuracy: 0.8261\n",
      "Epoch 299/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.5067 - accuracy: 0.8261\n",
      "Epoch 300/500\n",
      "171/171 [==============================] - 7s 43ms/step - loss: 0.5053 - accuracy: 0.8277\n",
      "Epoch 301/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.5042 - accuracy: 0.8270\n",
      "Epoch 302/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.5030 - accuracy: 0.8282\n",
      "Epoch 303/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5068 - accuracy: 0.8262\n",
      "Epoch 304/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.5014 - accuracy: 0.8285\n",
      "Epoch 305/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5031 - accuracy: 0.8285\n",
      "Epoch 306/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5011 - accuracy: 0.8289\n",
      "Epoch 307/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4983 - accuracy: 0.8296\n",
      "Epoch 308/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5016 - accuracy: 0.8288\n",
      "Epoch 309/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5008 - accuracy: 0.8293\n",
      "Epoch 310/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5005 - accuracy: 0.8286\n",
      "Epoch 311/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5020 - accuracy: 0.8293\n",
      "Epoch 312/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.5011 - accuracy: 0.8287\n",
      "Epoch 313/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4975 - accuracy: 0.8299\n",
      "Epoch 314/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4936 - accuracy: 0.8313\n",
      "Epoch 315/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4914 - accuracy: 0.8326\n",
      "Epoch 316/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4964 - accuracy: 0.8301\n",
      "Epoch 317/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4937 - accuracy: 0.8309\n",
      "Epoch 318/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4895 - accuracy: 0.8330\n",
      "Epoch 319/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4900 - accuracy: 0.8320\n",
      "Epoch 320/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4911 - accuracy: 0.8321\n",
      "Epoch 321/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4900 - accuracy: 0.8325\n",
      "Epoch 322/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4883 - accuracy: 0.8341\n",
      "Epoch 323/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4873 - accuracy: 0.8337\n",
      "Epoch 324/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4872 - accuracy: 0.8341\n",
      "Epoch 325/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4860 - accuracy: 0.8340\n",
      "Epoch 326/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4886 - accuracy: 0.8329\n",
      "Epoch 327/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4841 - accuracy: 0.8347\n",
      "Epoch 328/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4881 - accuracy: 0.8335\n",
      "Epoch 329/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4836 - accuracy: 0.8348\n",
      "Epoch 330/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4810 - accuracy: 0.8357\n",
      "Epoch 331/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4827 - accuracy: 0.8344\n",
      "Epoch 332/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4831 - accuracy: 0.8351\n",
      "Epoch 333/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4774 - accuracy: 0.8373\n",
      "Epoch 334/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4778 - accuracy: 0.8372\n",
      "Epoch 335/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4781 - accuracy: 0.8371\n",
      "Epoch 336/500\n",
      "171/171 [==============================] - 7s 43ms/step - loss: 0.4781 - accuracy: 0.8376\n",
      "Epoch 337/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4761 - accuracy: 0.8366\n",
      "Epoch 338/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4774 - accuracy: 0.8372\n",
      "Epoch 339/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4756 - accuracy: 0.8373\n",
      "Epoch 340/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4777 - accuracy: 0.8371\n",
      "Epoch 341/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4740 - accuracy: 0.8389\n",
      "Epoch 342/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4744 - accuracy: 0.8393\n",
      "Epoch 343/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4761 - accuracy: 0.8387\n",
      "Epoch 344/500\n",
      "171/171 [==============================] - 8s 45ms/step - loss: 0.4691 - accuracy: 0.8411\n",
      "Epoch 345/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4708 - accuracy: 0.8394\n",
      "Epoch 346/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4723 - accuracy: 0.8392\n",
      "Epoch 347/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4680 - accuracy: 0.8402\n",
      "Epoch 348/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4703 - accuracy: 0.8397\n",
      "Epoch 349/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4674 - accuracy: 0.8412\n",
      "Epoch 350/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4721 - accuracy: 0.8396\n",
      "Epoch 351/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4693 - accuracy: 0.8403\n",
      "Epoch 352/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4713 - accuracy: 0.8398\n",
      "Epoch 353/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4698 - accuracy: 0.8408\n",
      "Epoch 354/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4658 - accuracy: 0.8409\n",
      "Epoch 355/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4703 - accuracy: 0.8401\n",
      "Epoch 356/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4660 - accuracy: 0.8418\n",
      "Epoch 357/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4612 - accuracy: 0.8441\n",
      "Epoch 358/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4664 - accuracy: 0.8412\n",
      "Epoch 359/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4651 - accuracy: 0.8423\n",
      "Epoch 360/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4658 - accuracy: 0.8415\n",
      "Epoch 361/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4646 - accuracy: 0.8412\n",
      "Epoch 362/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4634 - accuracy: 0.8421\n",
      "Epoch 363/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4617 - accuracy: 0.8427\n",
      "Epoch 364/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4625 - accuracy: 0.8427\n",
      "Epoch 365/500\n",
      "171/171 [==============================] - 8s 45ms/step - loss: 0.4634 - accuracy: 0.8425\n",
      "Epoch 366/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4601 - accuracy: 0.8442\n",
      "Epoch 367/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4581 - accuracy: 0.8439\n",
      "Epoch 368/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.4572 - accuracy: 0.8447\n",
      "Epoch 369/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4596 - accuracy: 0.8442\n",
      "Epoch 370/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4591 - accuracy: 0.8443\n",
      "Epoch 371/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4530 - accuracy: 0.8464\n",
      "Epoch 372/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.4545 - accuracy: 0.8462\n",
      "Epoch 373/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.4579 - accuracy: 0.8449\n",
      "Epoch 374/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.4560 - accuracy: 0.8453\n",
      "Epoch 375/500\n",
      "171/171 [==============================] - 7s 38ms/step - loss: 0.4544 - accuracy: 0.8461\n",
      "Epoch 376/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.4550 - accuracy: 0.8451\n",
      "Epoch 377/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.4519 - accuracy: 0.8473\n",
      "Epoch 378/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.4511 - accuracy: 0.8474\n",
      "Epoch 379/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4483 - accuracy: 0.8479\n",
      "Epoch 380/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.4524 - accuracy: 0.8471\n",
      "Epoch 381/500\n",
      "171/171 [==============================] - 8s 45ms/step - loss: 0.4484 - accuracy: 0.8482\n",
      "Epoch 382/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4481 - accuracy: 0.8478\n",
      "Epoch 383/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4472 - accuracy: 0.8490\n",
      "Epoch 384/500\n",
      "171/171 [==============================] - 8s 44ms/step - loss: 0.4482 - accuracy: 0.8483\n",
      "Epoch 385/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.4485 - accuracy: 0.8473\n",
      "Epoch 386/500\n",
      "171/171 [==============================] - 7s 40ms/step - loss: 0.4464 - accuracy: 0.8488\n",
      "Epoch 387/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.4487 - accuracy: 0.8479\n",
      "Epoch 388/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4517 - accuracy: 0.8462\n",
      "Epoch 389/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4455 - accuracy: 0.8492\n",
      "Epoch 390/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4479 - accuracy: 0.8480\n",
      "Epoch 391/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4455 - accuracy: 0.8494\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4424 - accuracy: 0.8514\n",
      "Epoch 393/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4450 - accuracy: 0.8500\n",
      "Epoch 394/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4452 - accuracy: 0.8493\n",
      "Epoch 395/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4425 - accuracy: 0.8500\n",
      "Epoch 396/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4464 - accuracy: 0.8482\n",
      "Epoch 397/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4440 - accuracy: 0.8502\n",
      "Epoch 398/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4417 - accuracy: 0.8510\n",
      "Epoch 399/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4374 - accuracy: 0.8525\n",
      "Epoch 400/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4375 - accuracy: 0.8518\n",
      "Epoch 401/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4376 - accuracy: 0.8519\n",
      "Epoch 402/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4356 - accuracy: 0.8532\n",
      "Epoch 403/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4358 - accuracy: 0.8534\n",
      "Epoch 404/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4369 - accuracy: 0.8529\n",
      "Epoch 405/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4387 - accuracy: 0.8520\n",
      "Epoch 406/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4367 - accuracy: 0.8515\n",
      "Epoch 407/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4356 - accuracy: 0.8531\n",
      "Epoch 408/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4382 - accuracy: 0.8525\n",
      "Epoch 409/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4351 - accuracy: 0.8525\n",
      "Epoch 410/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4365 - accuracy: 0.8530\n",
      "Epoch 411/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4338 - accuracy: 0.8534\n",
      "Epoch 412/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4348 - accuracy: 0.8528\n",
      "Epoch 413/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4326 - accuracy: 0.8546\n",
      "Epoch 414/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4336 - accuracy: 0.8538\n",
      "Epoch 415/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4319 - accuracy: 0.8545\n",
      "Epoch 416/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4290 - accuracy: 0.8555\n",
      "Epoch 417/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4304 - accuracy: 0.8545\n",
      "Epoch 418/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4292 - accuracy: 0.8550\n",
      "Epoch 419/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4308 - accuracy: 0.8545\n",
      "Epoch 420/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4351 - accuracy: 0.8533\n",
      "Epoch 421/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4309 - accuracy: 0.8550\n",
      "Epoch 422/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4302 - accuracy: 0.8549\n",
      "Epoch 423/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4309 - accuracy: 0.8543\n",
      "Epoch 424/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4284 - accuracy: 0.8561\n",
      "Epoch 425/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4267 - accuracy: 0.8563\n",
      "Epoch 426/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4272 - accuracy: 0.8559\n",
      "Epoch 427/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.4284 - accuracy: 0.8560\n",
      "Epoch 428/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4282 - accuracy: 0.8564\n",
      "Epoch 429/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4265 - accuracy: 0.8567\n",
      "Epoch 430/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4261 - accuracy: 0.8565\n",
      "Epoch 431/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4201 - accuracy: 0.8591\n",
      "Epoch 432/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4241 - accuracy: 0.8577\n",
      "Epoch 433/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4285 - accuracy: 0.8562\n",
      "Epoch 434/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4221 - accuracy: 0.8576\n",
      "Epoch 435/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4235 - accuracy: 0.8579\n",
      "Epoch 436/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4240 - accuracy: 0.8572\n",
      "Epoch 437/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4220 - accuracy: 0.8583\n",
      "Epoch 438/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4255 - accuracy: 0.8573\n",
      "Epoch 439/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4247 - accuracy: 0.8571\n",
      "Epoch 440/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4225 - accuracy: 0.8578\n",
      "Epoch 441/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4226 - accuracy: 0.8580\n",
      "Epoch 442/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4224 - accuracy: 0.8584\n",
      "Epoch 443/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4173 - accuracy: 0.8596\n",
      "Epoch 444/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4156 - accuracy: 0.8604\n",
      "Epoch 445/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4194 - accuracy: 0.8592\n",
      "Epoch 446/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4126 - accuracy: 0.8617\n",
      "Epoch 447/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4199 - accuracy: 0.8591\n",
      "Epoch 448/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4166 - accuracy: 0.8595\n",
      "Epoch 449/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4154 - accuracy: 0.8604\n",
      "Epoch 450/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4185 - accuracy: 0.8594\n",
      "Epoch 451/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4199 - accuracy: 0.8591\n",
      "Epoch 452/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4241 - accuracy: 0.8589\n",
      "Epoch 453/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4197 - accuracy: 0.8593\n",
      "Epoch 454/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4190 - accuracy: 0.8598\n",
      "Epoch 455/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4175 - accuracy: 0.8605\n",
      "Epoch 456/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4193 - accuracy: 0.8593\n",
      "Epoch 457/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4186 - accuracy: 0.8591\n",
      "Epoch 458/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4127 - accuracy: 0.8619\n",
      "Epoch 459/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4178 - accuracy: 0.8597\n",
      "Epoch 460/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4148 - accuracy: 0.8602\n",
      "Epoch 461/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4134 - accuracy: 0.8613\n",
      "Epoch 462/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4142 - accuracy: 0.8611\n",
      "Epoch 463/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4138 - accuracy: 0.8615\n",
      "Epoch 464/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4098 - accuracy: 0.8630\n",
      "Epoch 465/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4112 - accuracy: 0.8623\n",
      "Epoch 466/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4134 - accuracy: 0.8606\n",
      "Epoch 467/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4112 - accuracy: 0.8628\n",
      "Epoch 468/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4094 - accuracy: 0.8627\n",
      "Epoch 469/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4085 - accuracy: 0.8631\n",
      "Epoch 470/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4070 - accuracy: 0.8624\n",
      "Epoch 471/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4087 - accuracy: 0.8633\n",
      "Epoch 472/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4072 - accuracy: 0.8638\n",
      "Epoch 473/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4078 - accuracy: 0.8635\n",
      "Epoch 474/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4074 - accuracy: 0.8638\n",
      "Epoch 475/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4070 - accuracy: 0.8635\n",
      "Epoch 476/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4074 - accuracy: 0.8634\n",
      "Epoch 477/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4072 - accuracy: 0.8632\n",
      "Epoch 478/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4133 - accuracy: 0.8618\n",
      "Epoch 479/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4036 - accuracy: 0.8649\n",
      "Epoch 480/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4044 - accuracy: 0.8651\n",
      "Epoch 481/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4073 - accuracy: 0.8639\n",
      "Epoch 482/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4050 - accuracy: 0.8647\n",
      "Epoch 483/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4075 - accuracy: 0.8633\n",
      "Epoch 484/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4173 - accuracy: 0.8612\n",
      "Epoch 485/500\n",
      "171/171 [==============================] - 7s 41ms/step - loss: 0.3995 - accuracy: 0.8668\n",
      "Epoch 486/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.4028 - accuracy: 0.8659\n",
      "Epoch 487/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4072 - accuracy: 0.8643\n",
      "Epoch 488/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4059 - accuracy: 0.8650\n",
      "Epoch 489/500\n",
      "171/171 [==============================] - 6s 37ms/step - loss: 0.4072 - accuracy: 0.8637\n",
      "Epoch 490/500\n",
      "171/171 [==============================] - 7s 39ms/step - loss: 0.4094 - accuracy: 0.8628\n",
      "Epoch 491/500\n",
      "171/171 [==============================] - 6s 38ms/step - loss: 0.4041 - accuracy: 0.8651\n",
      "Epoch 492/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4038 - accuracy: 0.8653\n",
      "Epoch 493/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4015 - accuracy: 0.8664\n",
      "Epoch 494/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4034 - accuracy: 0.8662\n",
      "Epoch 495/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.3983 - accuracy: 0.8670\n",
      "Epoch 496/500\n",
      "171/171 [==============================] - 6s 36ms/step - loss: 0.4014 - accuracy: 0.8664\n",
      "Epoch 497/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4021 - accuracy: 0.8661\n",
      "Epoch 498/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4017 - accuracy: 0.8653\n",
      "Epoch 499/500\n",
      "171/171 [==============================] - 6s 35ms/step - loss: 0.4021 - accuracy: 0.8665\n",
      "Epoch 500/500\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.4012 - accuracy: 0.8659\n"
     ]
    }
   ],
   "source": [
    "history = model_encoder_decoder.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee18d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dcb29e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "971c283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives one hot represetation and returns index where value = 1\n",
    "def one_hot_decode(arr):\n",
    "    for index,num in enumerate(arr):\n",
    "        if num == 1:\n",
    "            return index\n",
    "        \n",
    "# Receives an array to append to and a 3D-array that is one hot encoded      \n",
    "def decode(arr, three_d_array):\n",
    "    for seq in three_d_array:\n",
    "        temp = []\n",
    "        for one_hot in seq:\n",
    "            temp.append(one_hot_decode(one_hot))\n",
    "        arr.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "192f2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodes prediction done by LSTM and stores it in arr.\n",
    "def prediction_decode(arr, prediction):\n",
    "    for seq in prediction:\n",
    "        predict_temp = []\n",
    "        for one_hot in seq:\n",
    "            predict_temp.append(argmax(one_hot))\n",
    "        arr.append(predict_temp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d6da4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect match\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def calc_belu(target, prediction):\n",
    "    reference = []\n",
    "    candidate = []\n",
    "    reference.append(target)\n",
    "    candidate.extend(prediction)\n",
    "    return sentence_bleu(reference, candidate, weights=(0.3,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b349ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "decode(sources, x_test[0:100])\n",
    "\n",
    "targets = []\n",
    "decode(targets, y_test[0:100])\n",
    "\n",
    "predictions = []\n",
    "prediction_decode(predictions, model_encoder_decoder.predict( x_test[0:100], batch_size = batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4f73e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: [6, 63, 6, 11, 5, 192, 6, 192, 192, 125, 125, 125, 125, 54, 45]\n",
      "Target: [197, 197, 195, 174, 174, 3, 6, 91, 195, 3, 6, 91, 5, 45, 3]\n",
      "Prediction: [5, 3, 197, 197, 195, 3, 195, 195, 3, 6, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [175, 175, 45, 175, 175, 175, 175, 45, 45, 45, 45, 45, 175, 3, 175]\n",
      "Target: [175, 45, 45, 174, 174, 175, 120, 175, 3, 7, 7, 175, 175, 175, 120]\n",
      "Prediction: [175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175]\n",
      "---------------------------------------------------\n",
      "Source: [192, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197]\n",
      "Target: [192, 125, 192, 6, 192, 192, 243, 125, 125, 125, 125, 125, 125, 125, 125]\n",
      "Prediction: [192, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 6, 91, 192, 91, 168, 146, 168, 3, 3, 3, 168, 146]\n",
      "Target: [168, 3, 3, 3, 168, 146, 3, 3, 168, 168, 146, 168, 146, 146, 146]\n",
      "Prediction: [168, 3, 168, 146, 168, 3, 3, 168, 168, 195, 6, 91, 6, 91, 6]\n",
      "---------------------------------------------------\n",
      "Source: [91, 195, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3, 3]\n",
      "Target: [6, 91, 195, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3]\n",
      "Prediction: [6, 91, 195, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3]\n",
      "---------------------------------------------------\n",
      "Source: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78]\n",
      "Target: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78]\n",
      "Prediction: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78]\n",
      "---------------------------------------------------\n",
      "Source: [45, 45, 6, 5, 197, 3, 6, 5, 197, 45, 3, 45, 6, 5, 197]\n",
      "Target: [3, 6, 5, 197, 3, 6, 5, 197, 3, 6, 5, 197, 3, 6, 5]\n",
      "Prediction: [197, 3, 6, 3, 6, 6, 5, 6, 5, 6, 5, 6, 6, 5, 3]\n",
      "---------------------------------------------------\n",
      "Source: [4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Target: [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Prediction: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [175, 120, 175, 3, 3, 7, 175, 42, 120, 3, 3, 6, 175, 42, 175]\n",
      "Target: [3, 175, 175, 45, 45, 175, 175, 175, 120, 7, 175, 7, 119, 174, 175]\n",
      "Prediction: [175, 175, 7, 120, 7, 7, 119, 174, 175, 174, 175, 3, 3, 175, 6]\n",
      "---------------------------------------------------\n",
      "Source: [4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4]\n",
      "Target: [3, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3]\n",
      "Prediction: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------------------------\n",
      "Source: [5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33]\n",
      "Target: [5, 3, 197, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5]\n",
      "Prediction: [5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192, 192, 192, 6]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 168, 146, 168, 3, 3, 3, 168, 146, 168, 3, 3, 3, 168]\n",
      "Target: [146, 168, 3, 3, 3, 168, 146, 168, 3, 3, 3, 168, 146, 168, 3]\n",
      "Prediction: [146, 168, 3, 3, 3, 168, 3, 168, 3, 3, 3, 3, 146, 168, 3]\n",
      "---------------------------------------------------\n",
      "Source: [240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196]\n",
      "Target: [78, 265, 240, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265]\n",
      "Prediction: [78, 265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240]\n",
      "---------------------------------------------------\n",
      "Source: [54, 140, 54, 140, 221, 221, 42, 54, 140, 54, 140, 221, 221, 120, 6]\n",
      "Target: [6, 175, 174, 175, 3, 4, 3, 195, 195, 195, 195, 195, 195, 195, 195]\n",
      "Prediction: [42, 6, 175, 175, 4, 3, 3, 3, 3, 3, 3, 3, 3, 175, 175]\n",
      "---------------------------------------------------\n",
      "Source: [195, 5, 5, 5, 5, 195, 195, 195, 5, 5, 5, 5, 195, 195, 195]\n",
      "Target: [5, 5, 5, 5, 195, 195, 195, 5, 5, 5, 5, 6, 195, 195, 195]\n",
      "Prediction: [5, 5, 5, 5, 195, 195, 195, 5, 5, 5, 5, 3, 6, 195, 195]\n",
      "---------------------------------------------------\n",
      "Source: [240, 240, 174, 174, 175, 191, 122, 174, 45, 45, 199, 201, 200, 202, 5]\n",
      "Target: [197, 192, 192, 6, 5, 3, 6, 13, 85, 195, 195, 195, 195, 195, 54]\n",
      "Prediction: [197, 192, 192, 6, 5, 3, 6, 13, 85, 195, 195, 195, 195, 195, 54]\n",
      "---------------------------------------------------\n",
      "Source: [6, 6, 3, 6, 3, 6, 4, 6, 42, 54, 120, 6, 6, 175, 174]\n",
      "Target: [4, 3, 4, 3, 7, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Prediction: [54, 120, 3, 3, 4, 4, 3, 4, 4, 4, 4, 174, 7, 4, 4]\n",
      "---------------------------------------------------\n",
      "Source: [6, 221, 114, 4, 4, 4, 195, 195, 195, 195, 4, 195, 195, 195, 201]\n",
      "Target: [120, 114, 4, 4, 4, 195, 195, 195, 195, 4, 195, 195, 195, 201, 120]\n",
      "Prediction: [4, 4, 4, 4, 4, 4, 195, 195, 195, 120, 120, 220, 195, 195, 195]\n",
      "---------------------------------------------------\n",
      "Source: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Target: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Prediction: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "---------------------------------------------------\n",
      "Source: [4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Target: [4, 4, 4, 3, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4]\n",
      "Prediction: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------------------------\n",
      "Source: [221, 175, 195, 175, 175, 5, 175, 5, 175, 175, 175, 175, 175, 175, 175]\n",
      "Target: [195, 3, 175, 5, 33, 175, 175, 5, 175, 175, 175, 5, 63, 175, 5]\n",
      "Prediction: [175, 175, 175, 5, 175, 5, 175, 175, 175, 175, 195, 175, 195, 175, 195]\n",
      "---------------------------------------------------\n",
      "Source: [265, 78, 265, 265, 240, 265, 265, 265, 19, 4, 265, 4, 78, 78, 265]\n",
      "Target: [240, 240, 240, 265, 265, 78, 265, 265, 78, 265, 265, 265, 265, 265, 19]\n",
      "Prediction: [240, 240, 265, 240, 265, 265, 265, 78, 78, 265, 78, 78, 78, 78, 240]\n",
      "---------------------------------------------------\n",
      "Source: [3, 197, 192, 192, 6, 33, 5, 3, 197, 192, 192, 192, 192, 6, 33]\n",
      "Target: [5, 3, 197, 192, 192, 6, 192, 243, 125, 125, 125, 125, 125, 125, 125]\n",
      "Prediction: [5, 3, 197, 192, 192, 192, 6, 192, 243, 125, 125, 125, 125, 125, 125]\n",
      "---------------------------------------------------\n",
      "Source: [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "Target: [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "Prediction: [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 195, 5, 54, 140, 197, 221, 143, 3, 195, 195, 195, 3, 195]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [195, 195, 3, 3, 195, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 3, 45, 54, 140, 3, 54, 54, 140, 3]\n",
      "Target: [3, 175, 54, 140, 3, 3, 3, 3, 3, 3, 3, 3, 3, 175, 54]\n",
      "Prediction: [3, 3, 3, 3, 175, 54, 3, 3, 3, 3, 140, 140, 3, 3, 175]\n",
      "---------------------------------------------------\n",
      "Source: [192, 3, 197, 3, 6, 91, 6, 6, 6, 195, 5, 5, 5, 197, 5]\n",
      "Target: [197, 192, 3, 3, 3, 3, 3, 6, 91, 195, 195, 195, 195, 5, 6]\n",
      "Prediction: [197, 192, 3, 3, 3, 3, 6, 6, 91, 6, 5, 5, 5, 6, 5]\n",
      "---------------------------------------------------\n",
      "Source: [221, 221, 221, 11, 45, 33, 192, 33, 5, 197, 192, 6, 33, 5, 3]\n",
      "Target: [197, 192, 192, 6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3]\n",
      "Prediction: [197, 192, 192, 6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3]\n",
      "---------------------------------------------------\n",
      "Source: [240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196, 78]\n",
      "Target: [265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240, 240]\n",
      "Prediction: [265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240, 240]\n",
      "---------------------------------------------------\n",
      "Source: [45, 5, 3, 140, 5, 140, 3, 140, 140, 140, 78, 3, 140, 78, 78]\n",
      "Target: [78, 78, 78, 78, 78, 78, 78, 5, 33, 3, 3, 3, 6, 197, 45]\n",
      "Prediction: [78, 5, 78, 45, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [54, 140, 197, 3, 3, 197, 6, 174, 174, 174, 7, 174, 174, 174, 42]\n",
      "Target: [42, 120, 6, 6, 3, 6, 54, 140, 197, 3, 3, 197, 6, 174, 174]\n",
      "Prediction: [42, 120, 140, 6, 195, 195, 54, 140, 6, 195, 3, 3, 174, 174, 174]\n",
      "---------------------------------------------------\n",
      "Source: [125, 125, 91, 45, 45, 13, 33, 195, 5, 3, 33, 195, 5, 221, 141]\n",
      "Target: [141, 33, 195, 5, 3, 3, 6, 33, 195, 5, 3, 3, 6, 33, 195]\n",
      "Prediction: [141, 33, 195, 5, 3, 3, 6, 33, 195, 5, 5, 3, 6, 33, 195]\n",
      "---------------------------------------------------\n",
      "Source: [78, 3, 78, 142, 78, 3, 175, 175, 78, 142, 78, 175, 175, 175, 174]\n",
      "Target: [142, 142, 4, 174, 3, 78, 142, 78, 3, 78, 142, 78, 3, 175, 175]\n",
      "Prediction: [142, 174, 174, 142, 3, 78, 142, 78, 3, 78, 142, 78, 3, 175, 175]\n",
      "---------------------------------------------------\n",
      "Source: [168, 168, 102, 102, 168, 78, 240, 240, 240, 148, 140, 4, 148, 78, 4]\n",
      "Target: [195, 78, 102, 102, 102, 168, 168, 102, 102, 168, 168, 102, 102, 168, 168]\n",
      "Prediction: [102, 102, 168, 102, 168, 102, 168, 78, 168, 78, 168, 78, 78, 78, 240]\n",
      "---------------------------------------------------\n",
      "Source: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78]\n",
      "Target: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78]\n",
      "Prediction: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78]\n",
      "---------------------------------------------------\n",
      "Source: [5, 192, 33, 125, 6, 192, 6, 125, 125, 125, 125, 125, 125, 125, 125]\n",
      "Target: [125, 45, 13, 33, 5, 33, 141, 3, 3, 3, 3, 3, 3, 33, 3]\n",
      "Prediction: [125, 125, 125, 195, 5, 5, 5, 5, 221, 6, 6, 3, 5, 5, 5]\n",
      "---------------------------------------------------\n",
      "Source: [3, 197, 192, 192, 192, 6, 192, 243, 125, 125, 125, 91, 20, 174, 201]\n",
      "Target: [45, 45, 64, 195, 195, 5, 221, 6, 221, 174, 174, 174, 174, 174, 174]\n",
      "Prediction: [45, 45, 64, 195, 195, 5, 221, 6, 221, 174, 174, 174, 174, 174, 174]\n",
      "---------------------------------------------------\n",
      "Source: [146, 168, 3, 3, 3, 168, 146, 168, 3, 3, 3, 168, 146, 168, 3]\n",
      "Target: [3, 3, 168, 146, 168, 3, 3, 3, 168, 146, 168, 3, 3, 3, 168]\n",
      "Prediction: [3, 3, 168, 146, 221, 197, 192, 168, 168, 3, 168, 146, 168, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [78, 78, 78, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180]\n",
      "Target: [180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180]\n",
      "Prediction: [180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 3, 3, 6, 91, 5, 3, 3, 3, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 45, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Target: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Prediction: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Target: [6, 192, 192, 192, 192, 33, 3, 13, 13, 195, 174, 197, 192, 4, 13]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Target: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Prediction: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------------------------\n",
      "Source: [174, 174, 174, 174, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221]\n",
      "Target: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221]\n",
      "Prediction: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221]\n",
      "---------------------------------------------------\n",
      "Source: [168, 265, 102, 102, 102, 102, 265, 168, 265, 102, 221, 221, 102, 265, 265]\n",
      "Target: [265, 168, 265, 102, 102, 3, 265, 168, 265, 102, 265, 168, 265, 3, 265]\n",
      "Prediction: [265, 168, 168, 102, 102, 265, 168, 265, 168, 265, 3, 265, 168, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [221, 168, 168, 42, 4, 168, 4, 6, 4, 168, 4, 168, 168, 221, 4]\n",
      "Target: [168, 4, 168, 168, 102, 168, 102, 6, 6, 168, 4, 168, 42, 221, 221]\n",
      "Prediction: [4, 42, 42, 3, 4, 102, 4, 4, 168, 4, 168, 4, 168, 4, 168]\n",
      "---------------------------------------------------\n",
      "Source: [176, 162, 3, 176, 162, 3, 176, 162, 3, 176, 162, 3, 176, 162, 3]\n",
      "Target: [176, 162, 3, 176, 162, 3, 176, 162, 3, 176, 162, 3, 176, 162, 3]\n",
      "Prediction: [176, 162, 3, 176, 162, 3, 176, 162, 3, 176, 162, 3, 176, 162, 3]\n",
      "---------------------------------------------------\n",
      "Source: [102, 102, 13, 102, 102, 102, 102, 6, 195, 5, 197, 192, 3, 3, 6]\n",
      "Target: [91, 195, 102, 102, 78, 168, 102, 168, 54, 102, 6, 265, 78, 78, 265]\n",
      "Prediction: [91, 195, 102, 102, 78, 168, 102, 168, 54, 102, 6, 102, 102, 78, 78]\n",
      "---------------------------------------------------\n",
      "Source: [3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4]\n",
      "Target: [4, 3, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 3]\n",
      "Prediction: [4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------------------------\n",
      "Source: [5, 3, 3, 6, 33, 195, 5, 3, 33, 3, 6, 33, 195, 5, 3]\n",
      "Target: [3, 6, 33, 195, 5, 3, 3, 6, 33, 195, 5, 3, 3, 6, 33]\n",
      "Prediction: [3, 6, 33, 195, 5, 3, 3, 6, 33, 195, 5, 3, 3, 6, 33]\n",
      "---------------------------------------------------\n",
      "Source: [6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192, 192]\n",
      "Target: [192, 6, 192, 192, 243, 125, 125, 125, 125, 125, 125, 125, 125, 91, 258]\n",
      "Prediction: [6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192, 192]\n",
      "---------------------------------------------------\n",
      "Source: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Target: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Prediction: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "---------------------------------------------------\n",
      "Source: [195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 5, 197, 192, 197]\n",
      "Target: [140, 221, 221, 13, 5, 197, 197, 192, 3, 140, 3, 6, 91, 4, 5]\n",
      "Prediction: [192, 192, 6, 91, 5, 221, 197, 195, 4, 195, 5, 5, 5, 5, 5]\n",
      "---------------------------------------------------\n",
      "Source: [6, 6, 63, 63, 6, 6, 54, 140, 197, 221, 221, 174, 11, 45, 221]\n",
      "Target: [221, 221, 33, 33, 192, 33, 5, 197, 192, 6, 33, 5, 3, 197, 192]\n",
      "Prediction: [221, 221, 33, 33, 192, 33, 5, 197, 192, 6, 33, 5, 3, 197, 192]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 5, 5, 5, 197, 192, 3, 140, 3, 6, 91, 168, 168, 168]\n",
      "Target: [168, 146, 3, 3, 3, 3, 168, 146, 168, 146, 45, 45, 168, 45, 168]\n",
      "Prediction: [168, 168, 3, 168, 146, 168, 146, 3, 3, 168, 146, 168, 3, 3, 146]\n",
      "---------------------------------------------------\n",
      "Source: [3, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 3]\n",
      "Target: [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]\n",
      "Prediction: [3, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]\n",
      "---------------------------------------------------\n",
      "Source: [3, 195, 195, 3, 3, 6, 45, 3, 3, 6, 195, 5, 54, 140, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [45, 3, 3, 6, 3, 45, 3, 3, 3, 3, 3, 3, 3, 3, 195]\n",
      "---------------------------------------------------\n",
      "Source: [201, 195, 6, 196, 195, 6, 3, 195, 3, 6, 6, 6, 6, 6, 5]\n",
      "Target: [197, 197, 192, 45, 5, 197, 197, 192, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [5, 197, 6, 192, 6, 91, 6, 195, 5, 3, 6, 91, 5, 197, 3]\n",
      "---------------------------------------------------\n",
      "Source: [4, 140, 4, 140, 4, 140, 4, 140, 4, 140, 4, 140, 4, 140, 4]\n",
      "Target: [221, 221, 140, 4, 140, 4, 140, 4, 140, 3, 265, 140, 4, 265, 140]\n",
      "Prediction: [140, 4, 140, 4, 140, 4, 140, 4, 140, 4, 140, 4, 140, 4, 102]\n",
      "---------------------------------------------------\n",
      "Source: [192, 3, 6, 91, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192]\n",
      "Target: [3, 6, 91, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3]\n",
      "Prediction: [3, 6, 91, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3]\n",
      "---------------------------------------------------\n",
      "Source: [6, 5, 3, 6, 195, 5, 3, 6, 5, 3, 6, 195, 5, 3, 6]\n",
      "Target: [5, 3, 6, 195, 5, 3, 6, 5, 3, 6, 195, 5, 3, 6, 5]\n",
      "Prediction: [5, 3, 6, 195, 5, 3, 6, 5, 3, 6, 195, 5, 3, 6, 5]\n",
      "---------------------------------------------------\n",
      "Source: [102, 102, 168, 102, 168, 102, 102, 168, 102, 168, 102, 102, 168, 168, 102]\n",
      "Target: [102, 168, 168, 102, 102, 168, 102, 168, 102, 102, 168, 102, 168, 102, 102]\n",
      "Prediction: [102, 168, 102, 168, 168, 102, 102, 168, 102, 102, 168, 168, 102, 168, 168]\n",
      "---------------------------------------------------\n",
      "Source: [3, 168, 146, 168, 3, 3, 3, 168, 146, 3, 168, 3, 3, 168, 146]\n",
      "Target: [168, 3, 3, 168, 3, 3, 168, 146, 168, 3, 3, 3, 168, 146, 168]\n",
      "Prediction: [168, 168, 3, 3, 168, 146, 168, 3, 3, 146, 146, 168, 3, 45, 3]\n",
      "---------------------------------------------------\n",
      "Source: [221, 63, 120, 114, 63, 6, 63, 6, 5, 221, 6, 221, 63, 6, 120]\n",
      "Target: [114, 63, 6, 221, 6, 221, 63, 4, 63, 6, 120, 6, 3, 114, 114]\n",
      "Prediction: [114, 63, 114, 63, 6, 63, 6, 221, 221, 6, 221, 63, 6, 221, 6]\n",
      "---------------------------------------------------\n",
      "Source: [3, 6, 91, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3]\n",
      "Target: [6, 91, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3, 6]\n",
      "Prediction: [6, 91, 5, 197, 192, 3, 3, 6, 91, 195, 5, 197, 192, 3, 6]\n",
      "---------------------------------------------------\n",
      "Source: [168, 168, 4, 168, 221, 4, 42, 4, 168, 102, 168, 4, 168, 168, 3]\n",
      "Target: [102, 102, 4, 168, 168, 3, 4, 4, 168, 4, 168, 3, 102, 168, 102]\n",
      "Prediction: [102, 168, 168, 168, 102, 168, 102, 168, 168, 168, 168, 6, 6, 168, 4]\n",
      "---------------------------------------------------\n",
      "Source: [5, 195, 5, 3, 174, 168, 102, 168, 331, 159, 125, 158, 158, 4, 160]\n",
      "Target: [125, 120, 240, 5, 195, 195, 6, 192, 125, 91, 91, 220, 33, 125, 125]\n",
      "Prediction: [102, 168, 168, 102, 168, 102, 158, 331, 4, 265, 265, 265, 168, 102, 102]\n",
      "---------------------------------------------------\n",
      "Source: [265, 168, 168, 146, 3, 3, 265, 168, 3, 3, 265, 3, 3, 265, 168]\n",
      "Target: [265, 78, 78, 265, 3, 265, 168, 168, 146, 3, 3, 265, 168, 3, 3]\n",
      "Prediction: [265, 78, 78, 265, 3, 265, 168, 168, 146, 3, 3, 265, 168, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [4, 168, 168, 102, 102, 6, 6, 168, 4, 168, 221, 42, 221, 221, 221]\n",
      "Target: [221, 221, 221, 221, 168, 3, 102, 168, 102, 168, 102, 6, 6, 168, 4]\n",
      "Prediction: [221, 221, 221, 221, 168, 102, 168, 102, 6, 168, 168, 4, 168, 4, 168]\n",
      "---------------------------------------------------\n",
      "Source: [240, 240, 196, 240, 240, 78, 265, 240, 240, 265, 240, 240, 196, 78, 265]\n",
      "Target: [240, 240, 240, 196, 78, 240, 240, 196, 5, 196, 196, 196, 196, 240, 240]\n",
      "Prediction: [240, 240, 240, 196, 78, 265, 240, 240, 196, 78, 265, 240, 240, 240, 240]\n",
      "---------------------------------------------------\n",
      "Source: [6, 91, 240, 196, 78, 195, 195, 195, 195, 5, 195, 125, 125, 125, 91]\n",
      "Target: [45, 5, 5, 5, 5, 195, 5, 197, 192, 197, 140, 3, 197, 140, 3]\n",
      "Prediction: [268, 45, 3, 195, 5, 195, 195, 195, 196, 5, 196, 85, 196, 85, 33]\n",
      "---------------------------------------------------\n",
      "Source: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221]\n",
      "Target: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221]\n",
      "Prediction: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 63, 63]\n",
      "---------------------------------------------------\n",
      "Source: [265, 102, 102, 168, 102, 168, 265, 168, 102, 168, 102, 102, 168, 168, 102]\n",
      "Target: [102, 168, 102, 102, 168, 168, 102, 168, 265, 102, 265, 168, 102, 102, 168]\n",
      "Prediction: [168, 168, 168, 102, 168, 102, 168, 168, 102, 168, 168, 102, 168, 168, 168]\n",
      "---------------------------------------------------\n",
      "Source: [240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196]\n",
      "Target: [78, 265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240]\n",
      "Prediction: [78, 265, 240, 240, 240, 196, 78, 265, 240, 240, 240, 196, 78, 265, 240]\n",
      "---------------------------------------------------\n",
      "Source: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221]\n",
      "Target: [221, 221, 221, 221, 221, 221, 221, 221, 221, 220, 6, 5, 63, 6, 11]\n",
      "Prediction: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 221, 63, 63]\n",
      "---------------------------------------------------\n",
      "Source: [221, 220, 5, 3, 3, 5, 6, 3, 5, 195, 6, 195, 195, 197, 5]\n",
      "Target: [6, 6, 125, 125, 91, 91, 195, 5, 6, 33, 6, 197, 33, 125, 266]\n",
      "Prediction: [197, 192, 195, 91, 195, 6, 6, 192, 6, 192, 195, 3, 6, 192, 197]\n",
      "---------------------------------------------------\n",
      "Source: [45, 45, 45, 45, 91, 195, 5, 221, 197, 192, 6, 5, 5, 5, 5]\n",
      "Target: [5, 5, 5, 91, 195, 45, 45, 45, 91, 195, 5, 221, 197, 192, 6]\n",
      "Prediction: [5, 221, 45, 45, 45, 45, 45, 91, 195, 5, 221, 197, 192, 5, 45]\n",
      "---------------------------------------------------\n",
      "Source: [175, 175, 175, 174, 7, 175, 7, 119, 174, 175, 175, 175, 195, 175, 175]\n",
      "Target: [175, 175, 120, 175, 175, 175, 175, 174, 7, 175, 7, 119, 174, 175, 175]\n",
      "Prediction: [175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 7, 175, 175, 175]\n",
      "---------------------------------------------------\n",
      "Source: [221, 168, 3, 102, 168, 102, 168, 102, 6, 6, 168, 4, 168, 102, 168]\n",
      "Target: [168, 102, 168, 168, 42, 221, 221, 221, 221, 221, 221, 221, 221, 168, 168]\n",
      "Prediction: [168, 102, 168, 42, 221, 221, 221, 221, 221, 221, 221, 168, 3, 168, 168]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 19, 3, 19, 3, 3, 19, 19, 3, 3, 3, 3, 19, 19]\n",
      "Target: [3, 19, 19, 3, 3, 3, 19, 19, 3, 3, 3, 19, 19, 3, 3]\n",
      "Prediction: [19, 3, 19, 19, 3, 3, 3, 19, 19, 19, 19, 19, 19, 3, 19]\n",
      "---------------------------------------------------\n",
      "Source: [11, 45, 33, 5, 5, 192, 192, 5, 3, 192, 192, 33, 6, 192, 192]\n",
      "Target: [6, 6, 5, 6, 192, 3, 125, 125, 125, 125, 125, 125, 125, 91, 122]\n",
      "Prediction: [4, 5, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 33]\n",
      "---------------------------------------------------\n",
      "Source: [192, 6, 33, 5, 3, 197, 192, 192, 192, 6, 125, 125, 91, 5, 197]\n",
      "Target: [192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197, 192, 192]\n",
      "Prediction: [192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197, 192, 192]\n",
      "---------------------------------------------------\n",
      "Source: [102, 6, 6, 168, 4, 168, 42, 221, 221, 221, 221, 221, 221, 221, 221]\n",
      "Target: [168, 3, 102, 240, 240, 168, 240, 3, 240, 240, 240, 240, 240, 102, 168]\n",
      "Prediction: [168, 3, 102, 168, 102, 168, 102, 168, 168, 168, 4, 168, 42, 221, 221]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [192, 6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192]\n",
      "Target: [192, 192, 6, 192, 243, 125, 125, 125, 125, 125, 125, 125, 91, 258, 311]\n",
      "Prediction: [192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197, 192]\n",
      "---------------------------------------------------\n",
      "Source: [175, 3, 175, 195, 175, 5, 195, 195, 307, 3, 175, 5, 221, 175, 175]\n",
      "Target: [5, 221, 221, 221, 221, 175, 175, 195, 6, 175, 5, 175, 195, 3, 175]\n",
      "Prediction: [175, 33, 175, 33, 195, 195, 195, 175, 195, 175, 175, 175, 175, 175, 175]\n",
      "---------------------------------------------------\n",
      "Source: [195, 195, 195, 54, 140, 54, 140, 54, 140, 5, 54, 140, 221, 197, 174]\n",
      "Target: [45, 3, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195]\n",
      "Prediction: [3, 45, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195]\n",
      "---------------------------------------------------\n",
      "Source: [78, 168, 102, 78, 168, 102, 221, 221, 142, 4, 221, 78, 168, 102, 221]\n",
      "Target: [221, 142, 4, 221, 78, 78, 168, 102, 221, 221, 142, 4, 221, 78, 140]\n",
      "Prediction: [102, 168, 4, 4, 4, 78, 240, 102, 78, 102, 102, 13, 13, 102, 78]\n",
      "---------------------------------------------------\n",
      "Source: [6, 6, 197, 6, 6, 6, 5, 6, 6, 3, 6, 5, 6, 6, 45]\n",
      "Target: [6, 5, 6, 5, 6, 3, 45, 6, 5, 3, 6, 5, 197, 3, 6]\n",
      "Prediction: [6, 6, 6, 6, 45, 5, 6, 3, 6, 6, 6, 5, 3, 45, 3]\n",
      "---------------------------------------------------\n",
      "Source: [4, 4, 3, 3, 4, 3, 3, 45, 45, 3, 4, 3, 3, 45, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 33, 33, 33, 33]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 33, 33, 33, 33]\n",
      "---------------------------------------------------\n",
      "Source: [3, 6, 195, 195, 195, 195, 195, 195, 195, 3, 45, 3, 6, 195, 195]\n",
      "Target: [195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 5, 54, 140]\n",
      "Prediction: [195, 195, 195, 195, 195, 195, 5, 54, 140, 6, 195, 195, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [140, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 5, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 195, 195, 5, 3, 3, 6, 195, 3]\n",
      "Prediction: [197, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 6, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [192, 140, 91, 6, 3, 3, 6, 91, 6, 141, 6, 5, 141, 141, 6]\n",
      "Target: [13, 195, 33, 8, 6, 13, 102, 102, 102, 6, 13, 175, 174, 175, 162]\n",
      "Prediction: [141, 141, 5, 5, 5, 141, 6, 195, 5, 141, 141, 141, 6, 5, 141]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 6, 3, 3, 3, 3, 3, 45, 3, 3, 3, 3, 3, 3]\n",
      "Target: [3, 45, 3, 3, 3, 3, 3, 3, 3, 3, 45, 3, 3, 3, 3]\n",
      "Prediction: [3, 6, 3, 3, 3, 3, 3, 3, 45, 3, 3, 3, 3, 45, 45]\n",
      "---------------------------------------------------\n",
      "Source: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Target: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Prediction: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------------------------\n",
      "Source: [3, 6, 142, 7, 142, 7, 142, 142, 7, 142, 3, 6, 7, 174, 174]\n",
      "Target: [3, 6, 195, 6, 142, 142, 7, 3, 120, 3, 6, 6, 6, 7, 7]\n",
      "Prediction: [174, 174, 42, 120, 6, 3, 6, 120, 6, 174, 7, 120, 6, 195, 41]\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    print(f\"Source: {sources[i]}\")\n",
    "    print(f\"Target: {targets[i]}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f36faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = []\n",
    "for i in range(batch_size):\n",
    "    temp_score.append(calc_belu(targets[i],predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85fd5605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6665087973491287\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(temp_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ee35c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164bee48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5365a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_validation_data)):\n",
    "    for j in range(len(train_validation_data[i])):\n",
    "        train_validation_data[i][j] = int(train_validation_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b402d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_request_belu_score(request,start,end):\n",
    "    request_sources = []\n",
    "    request_targets = []\n",
    "    request_prediction = []\n",
    "    request_scores= []\n",
    "    \n",
    "    decoded_request_targets = []\n",
    "    \n",
    "    while(len(request)-start >= n+m):\n",
    "        request_sources.append(generate_one_hot(request[start:end]))\n",
    "        start += m\n",
    "        end += m\n",
    "        request_targets.append(request[start:end])\n",
    "    \n",
    "    i = 1\n",
    "    while(i < len(request_sources)):\n",
    "        prediction_decode( request_prediction, model_encoder_decoder.predict(request_sources[i-1:i]))\n",
    "        i += 1\n",
    "    \n",
    "    for i in range(len(request_prediction)):\n",
    "        request_scores.append(calc_belu(request_targets[i],request_prediction[i]))\n",
    "        \n",
    "    print(request_scores)\n",
    "    return request_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "123e1dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1072\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nInvalid input_h shape: [1,1,200] [1,100,200]\n\t [[{{node CudnnRNN}}]]\n\t [[model_encoder_decoder/decoder_lstm/PartitionedCall]] [Op:__inference_predict_function_383201]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_validation_data)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2300\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m means\u001b[38;5;241m.\u001b[39mappend(statistics\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcalc_request_belu_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_validation_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m))\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mcalc_request_belu_score\u001b[1;34m(request, start, end)\u001b[0m\n\u001b[0;32m     15\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(request_sources)):\n\u001b[1;32m---> 17\u001b[0m     prediction_decode( request_prediction, \u001b[43mmodel_encoder_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_sources\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(request_prediction)):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nInvalid input_h shape: [1,1,200] [1,100,200]\n\t [[{{node CudnnRNN}}]]\n\t [[model_encoder_decoder/decoder_lstm/PartitionedCall]] [Op:__inference_predict_function_383201]"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "\n",
    "for i in range(len(train_validation_data)-2300-1000):\n",
    "    print(f'{i+1}/{len(train_validation_data)-2300-1000}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    means.append(statistics.mean(calc_request_belu_score(train_validation_data[i],0,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_means = []\n",
    "\n",
    "for i in range((len(train_attack_data))):\n",
    "    print(f'{i+1}/{len(train_attack_data)}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    attack_means.append(statistics.mean(calc_request_belu_score(train_attack_data[i],0,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(means))\n",
    "print(statistics.mean(attack_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471d598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
