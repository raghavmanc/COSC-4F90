{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871a5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from itertools import chain\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.metrics import categorical_crossentropy\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239f500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unizipped Attack File\n"
     ]
    }
   ],
   "source": [
    "# Unzip Attack Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Attack_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Attack File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daccdbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unizipped Validation File\n"
     ]
    }
   ],
   "source": [
    "# Unzip Validation Folder to retrive subfolfers\n",
    "from zipfile import ZipFile\n",
    "file_name = \"Validation_Data_Master.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Unizipped Validation File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601b621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "# list that holds attack vectors\n",
    "train_attack_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Attack_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_attack_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "for i in range(len(train_attack_data)):\n",
    "  for j in range(len(train_attack_data[i])):\n",
    "    train_attack_data[i][j] = int(train_attack_data[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c0d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "# list that holds attack vectors\n",
    "train_validation_data = []\n",
    "\n",
    "# Set file path ot the folder for iterations\n",
    "input_dir = Path.cwd() / \"Validation_Data_Master\"\n",
    "# store all the files ending with .txt in a list called files\n",
    "files = list (input_dir.rglob(\"*.txt*\"))\n",
    "# Iterate over all the txt files and append the attack in the list train_attack data\n",
    "for FILE in files:\n",
    "  with open (FILE, 'r') as f:\n",
    "    stringData = (f.read())\n",
    "    string_split_data = stringData.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_validation_data.append(string_split_data)\n",
    "    \n",
    "\n",
    "for i in range(len(train_validation_data)):\n",
    "  for j in range(len(train_validation_data[i])):\n",
    "    train_validation_data[i][j] = int(train_validation_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21062d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4372"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b6e45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list holding normal training data\n",
    "train_normal_data = [] \n",
    "\n",
    "files = 834 # maximum number of files for training data\n",
    "main = \"UTD-0000\"\n",
    "# iterate over all files\n",
    "for i in range(1,files):\n",
    "  back = str(i)\n",
    "# appending each training vector to the list train_data.\n",
    "  with open (main[:len(main)-len(back)] + back  + \".txt\", 'r') as f:\n",
    "    string_data = (f.read())\n",
    "    string_split_data = string_data.split(\" \")\n",
    "    del (string_split_data[-1])\n",
    "    train_normal_data.append(string_split_data)\n",
    "\n",
    "for i in range(len(train_normal_data)):\n",
    "  for j in range(len(train_normal_data[i])):\n",
    "    train_normal_data[i][j] = int(train_normal_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eac1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOLD\n",
    "# Appending normal and attack vectors to training sample\n",
    "train_samples = []\n",
    "train_labels = []  # 0 stands for normal behaviour, 1 stands for anomolous\n",
    "\n",
    "for i in train_normal_data:\n",
    "  train_samples.append(i)\n",
    "  train_labels.append(0)  \n",
    "  \n",
    "\n",
    "for i in train_attack_data:\n",
    "  train_samples.append(i)\n",
    "  train_labels.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eefbe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Data\n",
    "vocab_size = 340 # Total unique number of system calls\n",
    "for i in range (len(train_samples)):\n",
    "  for j in range (len(train_samples[i])):\n",
    "       train_samples[i][j] /= vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e15270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW\n",
    "x_train = []\n",
    "y_train = []\n",
    "result = [0,1]\n",
    "#seq_y = tensorflow.keras.utils.to_categorical(result, num_classes = 2)\n",
    "\n",
    "# Accepts a request of variable length, number of sequences to make, and \n",
    "# index according to its behaviour.\n",
    "# This method splits the request in given number of system calls sequences and \n",
    "# tags with its respective label.\n",
    "def setup_Input(req,anomaly_switch):\n",
    "\n",
    "    if anomaly_switch == 0:\n",
    "      for i in range(train_samples):\n",
    "        x_train.append(train_samples[i])\n",
    "        y_train.append(0)\n",
    "    if anomaly_switch == 1:\n",
    "        for i in range(train_samples):\n",
    "            x_train.append()\n",
    "            y_train.append(1)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f69326",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "setup_Input() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m   setup_Input(train_samples[i],num_sequences,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m   \u001b[43msetup_Input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: setup_Input() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Normal Behaviour, -> INDEX [0]\n",
    "# Attack  -> INDEX [1]\n",
    "\n",
    "num_sequences = 15  # Number of sequences to divide the request in.\n",
    "\n",
    "for i in range(len(train_samples)):\n",
    "  if i > 832:\n",
    "    setup_Input(train_samples[i],num_sequences,1)\n",
    "  else:\n",
    "    setup_Input(train_samples[i],num_sequences,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d72f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting x_train and y_train into np arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test data into training and testing sets\n",
    "x_train, x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad46d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape = (x_train.shape[1:]), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate= 1e-2,decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer=opt, loss ='sparse_categorical_crossentropy', metrics= 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4168e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,y_train, batch_size = 200, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5643118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Accuracy\n",
    "score = model.evaluate(x_test,y_test, batch_size=128,)\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of the loss function \n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of the model on the test data\n",
    "predictions = model.predict(x_test, batch_size = 128,verbose= 0)\n",
    "rounded_predictions = np.argmax(predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01033d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize= False, title = \"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "  plt.imshow(cm,interpolation='nearest', cmap = cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes, rotation=45)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "\n",
    "  if normalize:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "      print(\"Confusion Matrix, without normalization\")\n",
    "  \n",
    "  print(cm)\n",
    "\n",
    "  thresh = cm.max() / 2\n",
    "  for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "    plt.text(j,i,cm[i,j],\n",
    "      horizontalalignment = 'center',\n",
    "      color = 'white' if cm[i,j] > thresh else 'black')\n",
    "    \n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel(\"True Label\")\n",
    "  plt.xlabel(\"Predicted Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['Attack_sys_call', 'Normal_sys_call']\n",
    "plot_confusion_matrix(cm = cm, classes = cm_plot_labels, title = 'Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74241257",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4210f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validate = []\n",
    "\n",
    "# Accepts a request of variable length, number of sequences.\n",
    "# This method splits the request in given number of system calls sequences \n",
    "def split_validation_request(req,n_features):\n",
    "  length = len(req)\n",
    "  length = int(length/n_features)\n",
    "  start = 0;\n",
    "  end = n_features;\n",
    "\n",
    "\n",
    "  for i in range(length):\n",
    "    seq_x = req[start:end]\n",
    "    x_validate.append(seq_x)\n",
    "    end += n_features;\n",
    "    start = end - n_features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3deb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Validation Data\n",
    "for i in range (len(train_validation_data)):\n",
    "  for j in range (len(train_validation_data[i])):\n",
    "       train_validation_data[i][j] /= vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400450ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_validation_data)):\n",
    "    split_validation_request(train_validation_data[i],num_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d77c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_predictions = model.predict(x_validate, batch_size = 128,verbose= 0)\n",
    "rounded_v_predictions = np.argmax(v_predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file using open() function\n",
    "file = open(\"predictions.txt\", 'w')\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "   \n",
    "for i in range(len(rounded_v_predictions)):\n",
    "    if(rounded_v_predictions[i] == 1):\n",
    "        incorrect+=1\n",
    "    else:\n",
    "        correct+=1\n",
    "    file.write(str(rounded_v_predictions[i])) \n",
    "    file.write(\" \")\n",
    "# closing the file\n",
    "file.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incorrect/(correct+incorrect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
